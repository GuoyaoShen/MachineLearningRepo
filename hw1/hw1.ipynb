{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tzPhMs9UhAFr"
      },
      "source": [
        "# Homework 1: Coding\n",
        "\n",
        "**Due Monday September 16th, 11:59pm.**\n",
        "\n",
        "**This is an individual assignment.**\n",
        "\n",
        "**Submit hw1.py file to Gradescope (you may submit as many times as you'd like before the deadline).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqy1LtZCIQuG",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Import libraries that you might require.\n",
        "\n",
        "DON'T comment out these imports when submitting your final hw1.py file.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HMQDN_YzIQuK"
      },
      "source": [
        "# Question 4: KNN Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WgH67-bV4AJ9"
      },
      "source": [
        "We will implement the KNN algorithm for the breast cancer dataset. Refer to the pdf and the following functions for the instructions. Complete all the functions as indicated below. The four functions would be autograded as mentioned in the pdf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5qmuIim2IQuK",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Task 1: Classification\n",
        "\n",
        "Please implement KNN for K: 3, 5, and 7 with the following norms:\n",
        "L1\n",
        "L2\n",
        "L-inf\n",
        "\"\"\"\n",
        "\n",
        "# Read data (Breast Cancer Dataset). Remember to comment out the code not contained in a function.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast = load_breast_cancer()\n",
        "\n",
        "X = breast['data']\n",
        "y = breast['target']\n",
        "\n",
        "np.random.seed(100)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "X_train, y_train = X[:400], y[:400]\n",
        "X_val, y_val = X[400:500], y[400:500]\n",
        "X_test, y_test = X[500:], y[500:]\n",
        "\n",
        "\n",
        "def distanceFunc(metric_type, vec1, vec2):\n",
        "    \"\"\"\n",
        "    Computes the distance between two d-dimension vectors. \n",
        "    \n",
        "    Please DO NOT use Numpy's norm function when implementing this function. \n",
        "    \n",
        "    Args:\n",
        "        metric_type (str): Metric: L1, L2, or L-inf\n",
        "        vec1 ((d,) np.ndarray): d-dim vector\n",
        "        vec2 ((d,) np.ndarray): d-dim vector\n",
        "    \n",
        "    Returns:\n",
        "        distance (float): distance between the two vectors\n",
        "    \"\"\"\n",
        "\n",
        "    diff = vec1 - vec2\n",
        "    diff = np.absolute(diff)\n",
        "    if metric_type == \"L1\":\n",
        "#         distance = 0 #complete\n",
        "        distance = np.sum(np.absolute(diff))\n",
        "\n",
        "    if metric_type == \"L2\":\n",
        "#         distance = 0 #complete\n",
        "        distance = np.sqrt(np.sum(np.square(diff)))\n",
        "        \n",
        "        \n",
        "    if metric_type == \"L-inf\":\n",
        "#         distance = 0 #complete\n",
        "        distance = np.amax(diff)\n",
        "        \n",
        "    return distance\n",
        "\n",
        "\n",
        "def computeDistancesNeighbors(K, metric_type, X_train, y_train, sample):\n",
        "    \"\"\"\n",
        "    Compute the distances between every datapoint in the train_data and the \n",
        "    given sample. Then, find the k-nearest neighbors.\n",
        "    \n",
        "    Return a numpy array of the label of the k-nearest neighbors.\n",
        "    \n",
        "    Args:\n",
        "        K (int): K-value\n",
        "        metric_type (str): metric type\n",
        "        X_train ((n,p) np.ndarray): Training data with n samples and p features\n",
        "        y_train : Training labels\n",
        "        sample ((p,) np.ndarray): Single sample whose distance is to computed with every entry in the dataset\n",
        "        \n",
        "    Returns:\n",
        "        neighbors (list): K-nearest neighbors' labels\n",
        "    \"\"\"\n",
        "\n",
        "    # You will also call the function \"distanceFunc\" here\n",
        "    # Complete this function\n",
        "    \n",
        "    # calculate distance\n",
        "    list_dist = []\n",
        "    for ele_train in X_train:\n",
        "        dist = distanceFunc(metric_type, ele_train, sample)\n",
        "        list_dist.append(dist)\n",
        "    list_arg = np.argsort(list_dist)\n",
        "    list_arg = list_arg[:K]\n",
        "\n",
        "    # get corresponding labels\n",
        "    neighbors = [y_train[i] for i in list_arg]\n",
        "    \n",
        "    return neighbors\n",
        "\n",
        "\n",
        "def Majority(neighbors):\n",
        "    \"\"\"\n",
        "    Performs majority voting and returns the predicted value for the test sample.\n",
        "    \n",
        "    Since we're performing binary classification the possible values are [0,1].\n",
        "    \n",
        "    Args:\n",
        "        neighbors (list): K-nearest neighbors' labels\n",
        "        \n",
        "    Returns:\n",
        "        predicted_value (int): predicted label for the given sample\n",
        "    \"\"\"\n",
        "    \n",
        "    # Performs majority voting\n",
        "    # Complete this function\n",
        "    \n",
        "    total = len(neighbors)\n",
        "    sucess = np.sum(np.array(neighbors))\n",
        "    fail = total - sucess\n",
        "\n",
        "    # judge\n",
        "    if sucess > fail:\n",
        "        predicted_value = 1\n",
        "    else:\n",
        "        predicted_value = 0\n",
        "    \n",
        "    return predicted_value\n",
        "\n",
        "\n",
        "def KNN(K, metric_type, X_train, y_train, X_val):\n",
        "    \"\"\"\n",
        "    Returns the predicted values for the entire validation or test set.\n",
        "    \n",
        "    Please DO NOT use Scikit's KNN model when implementing this function. \n",
        "\n",
        "    Args:\n",
        "        K (int): K-value\n",
        "        metric_type (str): metric type\n",
        "        X_train ((n,p) np.ndarray): Training data with n samples and p features\n",
        "        y_train : Training labels\n",
        "        X_val ((n, p) np.ndarray): Validation or test data\n",
        "        \n",
        "    Returns:\n",
        "        predicted_values (list): output for every entry in validation/test dataset \n",
        "    \"\"\"\n",
        "    \n",
        "    # Complete this function\n",
        "    # Loop through the val_data or the test_data (as required)\n",
        "    # and compute the output for every entry in that dataset  \n",
        "    # You will also call the function \"Majority\" here\n",
        "    \n",
        "    # loop through val/test set\n",
        "    predictions = []\n",
        "    for ele_val in X_val:\n",
        "        neighbors = computeDistancesNeighbors(K, metric_type, X_train, y_train, ele_val)\n",
        "        pred = Majority(neighbors)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluation(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values ((n,) np.ndarray): Predicted values for n samples\n",
        "        actual_values ((n,) np.ndarray): Actual values for n samples\n",
        "    \n",
        "    Returns:\n",
        "        accuracy (float): accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    return accuracy_score(predicted_values, actual_values)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Calls the above functions in order to implement the KNN algorithm.\n",
        "    \n",
        "    Test over the following range K = 3,5,7 and all three metrics. \n",
        "    In total you will have nine combinations to try.\n",
        "    \n",
        "    PRINTS out the accuracies for the nine combinations on the validation set,\n",
        "    and the accuracy on the test set for the selected K value and appropriate norm.\n",
        "    \n",
        "    REMEMBER: You have to report these values by populating the Table 2 in the latex file.\n",
        "    \"\"\"\n",
        "    \n",
        "    ## Complete this function\n",
        "    \n",
        "    K = [3,5,7]\n",
        "    norm = [\"L1\", \"L2\", \"L-inf\"]\n",
        "    \n",
        "    print(\"<<<<VALIDATION DATA PREDICTIONS>>>>\")\n",
        "    \n",
        "    ## Complete\n",
        "    for ele_K in K:\n",
        "        for ele_norm in norm:\n",
        "            ele_pred = KNN(ele_K, ele_norm, X_train, y_train, X_val)\n",
        "            ele_acc = evaluation(ele_pred, y_val)\n",
        "            print('accuracy for K=' + str(ele_K) + ', norm=' + ele_norm + ':' + str(ele_acc))\n",
        "#     predictions = KNN(K[0], norm[1], X_train, y_train, X_val)\n",
        "#     predictions = KNN(K[2], norm[1], X_train, y_train, X_test)\n",
        "#     print('predictions:', predictions)\n",
        "    \n",
        "    print(\"<<<<TEST DATA PREDICTIONS>>>>\")\n",
        "    \n",
        "    ## Complete\n",
        "    for ele_K in K:\n",
        "        for ele_norm in norm:\n",
        "            ele_pred = KNN(ele_K, ele_norm, X_train, y_train, X_test)\n",
        "            ele_acc = evaluation(ele_pred, y_test)\n",
        "            print('accuracy for K=' + str(ele_K) + ', norm=' + ele_norm + ':' + str(ele_acc))\n",
        "#     acc = evaluation(predictions, y_val)\n",
        "#     acc = evaluation(predictions, y_test)\n",
        "#     print('accuracy:', acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n45LQ_db9HdO"
      },
      "source": [
        "Uncomment the code below to run the main function (Remember to recomment the code before submitting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DW4D4AbBjhK5",
        "outputId": "ba3a1449-f703-404f-91fa-d8acaf5e88b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Finally, call the main function\n",
        "main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<<<<VALIDATION DATA PREDICTIONS>>>>\n",
            "accuracy for K=3, norm=L1:0.94\n",
            "accuracy for K=3, norm=L2:0.95\n",
            "accuracy for K=3, norm=L-inf:0.94\n",
            "accuracy for K=5, norm=L1:0.94\n",
            "accuracy for K=5, norm=L2:0.93\n",
            "accuracy for K=5, norm=L-inf:0.94\n",
            "accuracy for K=7, norm=L1:0.93\n",
            "accuracy for K=7, norm=L2:0.92\n",
            "accuracy for K=7, norm=L-inf:0.93\n",
            "<<<<TEST DATA PREDICTIONS>>>>\n",
            "accuracy for K=3, norm=L1:0.8840579710144928\n",
            "accuracy for K=3, norm=L2:0.8840579710144928\n",
            "accuracy for K=3, norm=L-inf:0.8985507246376812\n",
            "accuracy for K=5, norm=L1:0.9130434782608695\n",
            "accuracy for K=5, norm=L2:0.8985507246376812\n",
            "accuracy for K=5, norm=L-inf:0.8985507246376812\n",
            "accuracy for K=7, norm=L1:0.8985507246376812\n",
            "accuracy for K=7, norm=L2:0.9130434782608695\n",
            "accuracy for K=7, norm=L-inf:0.8985507246376812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yRHmX6xYlEMO"
      },
      "source": [
        "### Question 4 Report in LaTeX\n",
        "\n",
        "Answer the following questions in your LaTeX document:\n",
        "\n",
        "1. How could having a larger dataset influence the performance of KNN?\n",
        "\n",
        "2. Tabulate your results from `main()` in the table provided in the LaTeX template.\n",
        "\n",
        "3. Finally, mention the best K and the norm combination you have settled upon and report the accuracy on the test set using that combination.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XmURULzoXCvq"
      },
      "source": [
        "# Question 5: Decision Tree Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S-_QAM3jXCvq"
      },
      "source": [
        "### Helper functions\n",
        "The block below contains helper functions for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9KC2aNmoXCvr",
        "colab": {}
      },
      "source": [
        "# Below are a list of helper functions to use to help you on this task\n",
        "\n",
        "def train_decision_tree(X, y, depth=None, leaf_count=None):\n",
        "    \"\"\"\n",
        "    Trains a decision tree classifier on the given X, y data with the specified \n",
        "    tree depth d and max leaf node count max_leaf_num.\n",
        "\n",
        "    Args:\n",
        "    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                          p (number of features) matrix\n",
        "    y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "    depth (int): The maximum depth of the tree. A value of None means no restrictions\n",
        "             on the depth of the tree.\n",
        "    leaf_count (int): The maximum leaf count of the tree's leaf nodes. A value of None means \n",
        "    no restrictions on the leaf count of the tree.\n",
        "\n",
        "    Returns:\n",
        "    clf(DecisionTreeClassifier): the trained decision tree classifier\n",
        "    \"\"\"\n",
        "\n",
        "    clf = DecisionTreeClassifier(max_depth=depth, max_leaf_nodes=leaf_count, criterion=\"entropy\", random_state=1)\n",
        "    clf.fit(X,y)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def predict(clf, X_test):\n",
        "    \"\"\"\n",
        "    Uses a trained decision tree classifier to predict on a given test set.\n",
        "\n",
        "    Args:\n",
        "    clf (DecisionTreeClassifier): Trained Decision Tree Classifer\n",
        "    X_test ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                               p (number of features) matrix\n",
        "\n",
        "    Returns:\n",
        "    y_pred ((n,) np.ndarray): The output predictions, which are in an n length array\n",
        "    \"\"\"\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "def evaluate(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values: numpy array\n",
        "        actual_values: numpy array\n",
        "    \n",
        "    Returns:\n",
        "        a floating point number representing the accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    from sklearn.metrics import accuracy_score\n",
        "    return accuracy_score(predicted_values, actual_values)\n",
        "  \n",
        "    \n",
        "def plot_line_graph(x_vals, y_vals_1, y_vals_2, y_vals_1_label, y_vals_2_label, x_axis_label, y_axis_label, title):\n",
        "    \"\"\"\n",
        "    Plots a line graph of two lines of different values with common x-values\n",
        "\n",
        "    Args:\n",
        "    x_vals ((j,) list): Values to be displayed on horizontal axis, where j is number of values\n",
        "    y_vals_1 ((j,) list): First set of values to be graphed on a line in respect to x_vals, where j is number of values\n",
        "    y_vals_2 ((j,) list): Second set of values to be graphed on a line in respect to x_vals, where j is number of values\n",
        "    y_vals_1_label (string): Label for first set of y values\n",
        "    y_vals_2_label (string): Label for second set of y values\n",
        "    x_axis_label (string): Label for x axis\n",
        "    y_axis_label (string): Label for y axis\n",
        "    title (string): Plot title\n",
        "    \"\"\" \n",
        "\n",
        "    plt.plot(x_vals, y_vals_1, color='g', label=y_vals_1_label)\n",
        "    plt.plot(x_vals, y_vals_2, color='orange', label=y_vals_2_label)\n",
        "    plt.xlabel(x_axis_label)\n",
        "    plt.ylabel(y_axis_label)\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eP9j8K21XCvs"
      },
      "source": [
        "### Compare Accuracy for full classification dataset as well as smaller classification dataset\n",
        "We will be using the breast cancer classification dataset. You are also given a smaller training dataset with the same data as the full dataset but with only half of the sample number. We will observe the performance changes when less data is available.\n",
        "\n",
        "To start, uncomment the code below and run to retrieve the data. (Recomment before submission)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LRothkZeXCvt",
        "colab": {}
      },
      "source": [
        "# We will also use the same breast cancer classification dataset in Task 1.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "breast = load_breast_cancer()\n",
        "\n",
        "X = breast['data']\n",
        "y = breast['target']\n",
        "\n",
        "np.random.seed(100)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "X_train, y_train = X[:400], y[:400]\n",
        "X_val, y_val = X[400:500], y[400:500]\n",
        "X_test, y_test = X[500:], y[500:]\n",
        "\n",
        "# Let's create a smaller version of the training dataset using only half of the data available\n",
        "train_sample_num_small = int(X_train.shape[0] / 2)\n",
        "X_train_small, y_train_small = X_train[:train_sample_num_small], y_train[:train_sample_num_small]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PEsSQwRMXCvv"
      },
      "source": [
        "### Base Metrics on Full and Partial Data\n",
        "To start, you will be comparing the training and testing accuracies of both datasets given a vanilla decision tree.\n",
        "\n",
        "Note: Make sure to create two separate classifiers for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xfRXN92DXCvv",
        "colab": {}
      },
      "source": [
        "def base_metrics(X_train, y_train, X_train_small, y_train_small, X_val, y_val, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Create a decision tree classifer on the full dataset and the partial dataset (only half of n).\n",
        "\n",
        "    Args: (Note that n is not the same among train and test sets, but merely refers to sample size)\n",
        "        X_train ((n,p) np.ndarray): Input feature matrix of full dataset for training/fitting\n",
        "        y_train ((n,) np.ndarray): Input label array of full dataset for training/fitting\n",
        "        X_train_small ((n,p) np.ndarray): Input feature matrix of partial/small dataset for training/fitting\n",
        "        y_train_small ((n,) np.ndarray): Input label array of partial/small dataset for training/fitting\n",
        "        X_val ((n,p) np.ndarray): Input feature matrix of full dataset for validation\n",
        "        y_val ((n,) np.ndarray): Input label array of full dataset for validation\n",
        "        X_test ((n,p) np.ndarray): Input feature matrix of full dataset for testing\n",
        "        y_test ((n,) np.ndarray): Input label array of full dataset for testing    \n",
        "\n",
        "    Returns:\n",
        "        train_acc_full_set (float): Training accuracy using a model trained on the full dataset\n",
        "        val_acc_full_set (float): Validation accuracy using a model trained on the full dataset\n",
        "        test_acc_full_set (float): Test accuracy using a model trained on the full dataset\n",
        "        train_acc_small_set (float): Training accuracy using a model trained on the small dataset\n",
        "        val_acc_small_set (float): Validation accuracy using a model trained on the small dataset\n",
        "        test_acc_small_set (float): Test accuracy using a model trained on the small dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # <---- Your code here ----->\n",
        "    \n",
        "    # training based on original dataset\n",
        "    clf = train_decision_tree(X_train, y_train)\n",
        "    \n",
        "    pred_train = predict(clf, X_train)\n",
        "    pred_val = predict(clf, X_val)\n",
        "    pred_test = predict(clf, X_test)\n",
        "    \n",
        "    train_acc_full_set = evaluate(pred_train, y_train)\n",
        "    val_acc_full_set = evaluate(pred_val, y_val)\n",
        "    test_acc_full_set = evaluate(pred_test, y_test)\n",
        "    \n",
        "    \n",
        "    # training based on small dataset\n",
        "    clf_small = train_decision_tree(X_train_small, y_train_small)\n",
        "    \n",
        "    pred_small_train = predict(clf_small, X_train_small)\n",
        "    pred_small_val = predict(clf_small, X_val)\n",
        "    pred_small_test = predict(clf_small, X_test)\n",
        "    \n",
        "    train_acc_small_set = evaluate(pred_small_train, y_train_small)\n",
        "    val_acc_small_set = evaluate(pred_small_val, y_val)\n",
        "    test_acc_small_set = evaluate(pred_small_test, y_test)\n",
        "\n",
        "    # <---- Your code here ----->\n",
        "\n",
        "    print(\"Train Accuracy on Full Dataset: \", train_acc_full_set)\n",
        "    print(\"Validation Accuracy on Full Dataset: \", val_acc_full_set)\n",
        "    print(\"Test Accuracy on Full Dataset: \", test_acc_full_set)\n",
        "    print(\"Train Accuracy on Small (Half) Dataset: \", train_acc_small_set)\n",
        "    print(\"Validation Accuracy on Small (Half) Dataset: \", val_acc_small_set)\n",
        "    print(\"Test Accuracy on Small (Half) Dataset: \", test_acc_small_set)\n",
        "\n",
        "    return (train_acc_full_set, \n",
        "          val_acc_full_set, \n",
        "          test_acc_full_set,\n",
        "          train_acc_small_set, \n",
        "          val_acc_small_set, \n",
        "          test_acc_small_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QTH2bUBY3SCP"
      },
      "source": [
        "Uncomment the code below and run the code. (Remember to recomment the code before submitting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VM9BV2i3XCvx",
        "outputId": "e29915c2-8243-42da-ae9d-0aa54c974e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "base_metrics(X_train, y_train, X_train_small, y_train_small, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy on Full Dataset:  1.0\n",
            "Validation Accuracy on Full Dataset:  0.92\n",
            "Test Accuracy on Full Dataset:  0.9130434782608695\n",
            "Train Accuracy on Small (Half) Dataset:  1.0\n",
            "Validation Accuracy on Small (Half) Dataset:  0.91\n",
            "Test Accuracy on Small (Half) Dataset:  0.8840579710144928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0.92, 0.9130434782608695, 1.0, 0.91, 0.8840579710144928)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7gUUoJSO3SCR"
      },
      "source": [
        "### Question 5.1 Report on LaTeX\n",
        "Answer the following questions on LaTeX in the respective section.\n",
        "1. Report the results of the accuracies on LateX.\n",
        "2. Which dataset had a higher difference between training and test accuracy? Briefly explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6xiF5-LvXCvz"
      },
      "source": [
        "### Improving Decision Tree for Smaller Dataset by Tuning Hyperparameters\n",
        "Classifiers often overfit on smaller datasets, so now, we will optimize hyperparameters on tree depth and max leaf count to improve the performance of our model. \n",
        "\n",
        "Fill out the helper functions below which will take an array of hyperparameter values for tree depth and an array of hyperparameter values for max leaf count. The helper function will return a training and validation accuracy score for every pair of hyperparameter values. This is referred to as **grid search** for hyperparameter tuning.\n",
        "\n",
        "At the end, the function identifies the best value of the tree depth and tree node count hyperparameters for a dataset, as well as the final training and testing scores.\n",
        "\n",
        "Note: Use the highest validation score to choose the optimal hyperparameter combination. If there is a tie, use the lower hyperparameter value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k1nPIPFmXCvz",
        "colab": {}
      },
      "source": [
        "def grid_search_depth_and_leaf_count(depth_search_space, leaf_count_search_space, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Perform a decision tree hyperparameter grid search on tree depth and leaf count given training and validation data.\n",
        "\n",
        "    Please do not use Numpy's norm function when implementing this function. \n",
        "\n",
        "\n",
        "    Args:\n",
        "        depth_search_space ((d,) list): Tree depth values to search over, i.e. [1, 3, 6, 10, 30]\n",
        "        leaf_count_search_space ((l,) list): Max leaf count values to search over, i.e. [2, 3, 4, 5, 6]\n",
        "        X_train ((n, p) np.ndarray): The input feature matrix for training\n",
        "        y_train ((n, p) np.ndarray): The input ys for training\n",
        "        X_val ((n, p) np.ndarray): The input feature matrix that will be used to validate accuracy scores\n",
        "        y_val ((n, p) np.ndarray): The input ys that will be used to validate accuracy scores\n",
        "\n",
        "    Returns:\n",
        "        best_depth (int): The depth count in the hyperparameter combination with the largest validation score\n",
        "        best_leaf_count (int): The leaf count in the hyperparameter combination with the largest validation score\n",
        "    \"\"\"\n",
        "\n",
        "    # <---- Your code here ----->\n",
        "    \n",
        "    list_acc = []\n",
        "    list_depth = []\n",
        "    list_leaf = []\n",
        "    # search all combinations\n",
        "    for ele_depth in depth_search_space:\n",
        "        for ele_leaf in leaf_count_search_space:\n",
        "            # train model and get accuracy\n",
        "            ele_clf = train_decision_tree(X_train, y_train, depth=ele_depth, leaf_count=ele_leaf)\n",
        "            ele_pred = predict(ele_clf, X_val)\n",
        "            ele_acc = evaluate(ele_pred, y_val)\n",
        "            list_acc.append(ele_acc)\n",
        "            list_depth.append(ele_depth)\n",
        "            list_leaf.append(ele_leaf)\n",
        "    # get best combination\n",
        "    idx = np.argmax(list_acc)\n",
        "    best_depth = list_depth[idx]\n",
        "    best_leaf_count = list_leaf[idx]\n",
        "\n",
        "    # <---- Your code here ----->\n",
        "\n",
        "    print(\"Chosen Depth: \", best_depth)\n",
        "    print(\"Chosen Leaf: \", best_leaf_count)\n",
        "\n",
        "    return best_depth, best_leaf_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RvCBkKLJ3SCV"
      },
      "source": [
        "Uncomment and run the code below to and record the best depth and best leaf count hyperparameters on LaTeX in the respective section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RyZZ2CHb3SCW",
        "outputId": "00f0185d-3d11-4c51-df8c-2b20ee25f983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Search spaces for grid search to tune tree depth and leaf count hyperparameters\n",
        "depth_search_space = [2, 4, 6, 8, 10, 16, 20]\n",
        "leaf_count_search_space = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "print(\"FULL DATASET\")\n",
        "grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                 leaf_count_search_space, \n",
        "                                 X_train, \n",
        "                                 y_train, \n",
        "                                 X_val, \n",
        "                                 y_val)\n",
        "\n",
        "print(\"\\nSMALL DATASET\")\n",
        "grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                 leaf_count_search_space, \n",
        "                                 X_train_small, \n",
        "                                 y_train_small, \n",
        "                                 X_val,\n",
        "                                 y_val)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL DATASET\n",
            "Chosen Depth:  4\n",
            "Chosen Leaf:  5\n",
            "\n",
            "SMALL DATASET\n",
            "Chosen Depth:  2\n",
            "Chosen Leaf:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T_oY8swF3SCX"
      },
      "source": [
        "### Question 5.2 Report in LaTeX\n",
        "Answer the following questions in your LaTeX document.\n",
        "1. Report the chosen hyperparameters for both the complete set and the partial set.\n",
        "2. Did the small dataset have higher or lower chosen hyperparameter values than the full dataset? Briefly explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e85OLhHfXCv_"
      },
      "source": [
        "### Retrain Decision Tree and Plot Hyperparameter Search\n",
        "Now retrain your decision tree with the optimal hyperparameters. Report training, validation, and testing error for the small dataset.\n",
        "\n",
        "Also for the small dataset, create a graph plotting the training and validation scores for each leaf node hyperparameter value, holding the tree depth hyperparameter consistent at the chosen value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JrWrji_iXCwA",
        "colab": {}
      },
      "source": [
        "def retrain_decision_tree(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Perform a decision tree hyperparameter grid search given training and validation \n",
        "    data and search values for tree depth and leaf node count.\n",
        "\n",
        "    We also train decision trees by iterating over the entire leaf count space\n",
        "    and holding the chosen_depth constant to observe the change in accuracy.\n",
        "    \n",
        "    Args: (Note that n is not the same among train and test sets, but merely refers to sample size)\n",
        "        X_train ((n,p) np.ndarray)\n",
        "        y_train ((n,) np.ndarray)\n",
        "        X_val ((n,p) np.ndarray)\n",
        "        y_val ((n,) np.ndarray)\n",
        "        X_test ((n,p) np.ndarray)\n",
        "        y_test ((n,) np.ndarray)\n",
        "\n",
        "    Returns:\n",
        "        train_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "        val_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "        test_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "\n",
        "        leaf_count_train_scores (list): Report training scores for the entire leaf \n",
        "                                        count search space, holding chosen_depth constant\n",
        "        leaf_count_val_scores (list): Report validation scores for the entire leaf \n",
        "                                      count search space, holding chosen_depth constant\n",
        "    \"\"\"\n",
        "\n",
        "    # Select best hyperparameters\n",
        "    depth_search_space = [2, 4, 6, 8, 10, 16, 20]\n",
        "    leaf_count_search_space = [2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
        "\n",
        "    chosen_depth, chosen_leaf_count = grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                                                     leaf_count_search_space, \n",
        "                                                                     X_train, \n",
        "                                                                     y_train, \n",
        "                                                                     X_val, \n",
        "                                                                     y_val)\n",
        "\n",
        "    # <---- Your code here ----->  \n",
        "    \n",
        "    # ====== PART 1 ======\n",
        "    # reconstruct decision tree\n",
        "    clf_optim = train_decision_tree(X_train, y_train, depth=chosen_depth, leaf_count=chosen_leaf_count)\n",
        "    # get prediction\n",
        "    pred_optim_train = predict(clf_optim, X_train)\n",
        "    pred_optim_val = predict(clf_optim, X_val)\n",
        "    pred_optiml_test = predict(clf_optim, X_test)\n",
        "    #get accuracy\n",
        "    train_acc = evaluate(pred_optim_train, y_train)\n",
        "    val_acc = evaluate(pred_optim_val, y_val)\n",
        "    test_acc = evaluate(pred_optiml_test, y_test)\n",
        "    \n",
        "    # ====== PART 2 ======\n",
        "    leaf_count_train_scores = []\n",
        "    leaf_count_val_scores = []\n",
        "    for ele_leaf in leaf_count_search_space:\n",
        "        # construct model\n",
        "        ele_clf = train_decision_tree(X_train, y_train, depth=chosen_depth, leaf_count=ele_leaf)\n",
        "        # get prediction\n",
        "        ele_pred_train = predict(ele_clf, X_train)\n",
        "        ele_pred_val = predict(ele_clf, X_val)\n",
        "        # get accuracy\n",
        "        ele_acc_train = evaluate(ele_pred_train, y_train)\n",
        "        ele_acc_val = evaluate(ele_pred_val, y_val)\n",
        "        # update list\n",
        "        leaf_count_train_scores.append(ele_acc_train)\n",
        "        leaf_count_val_scores.append(ele_acc_val)\n",
        "\n",
        "    # <---- Your code here ----->\n",
        "\n",
        "    print(\"Optimal Hyperparameters Train Accuracy: \", train_acc)\n",
        "    print(\"Optimal Hyperparameters Validation Accuracy: \", val_acc)\n",
        "    print(\"Optimal Hyperparameters Test Accuracy: \", test_acc)\n",
        "\n",
        "    print(\"Training Scores per Max Leaf Node Count:\", leaf_count_train_scores)\n",
        "    print(\"Validation Scores per Max Leaf Node Count:\", leaf_count_val_scores)\n",
        "\n",
        "    return(train_acc, val_acc, test_acc, leaf_count_train_scores, leaf_count_val_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F0BXjFYN3SCa"
      },
      "source": [
        "Run the code above for the small dataset and by uncommenting code below. Report all necessary values and both graphs on Latex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jLJCMcNWXCwD",
        "outputId": "f7162ab7-d5e4-460c-949f-19bd2c630b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "(train_acc, val_acc, test_acc, leaf_count_train_scores, leaf_count_val_scores) = retrain_decision_tree(X_train_small, y_train_small, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# plot\n",
        "leaf_count_search_space = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "plot_line_graph(leaf_count_search_space, leaf_count_train_scores, leaf_count_val_scores, 'train_accuracy', 'val_accuracy', 'leaf_value', 'acc_value', 'leaf-train-val')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chosen Depth:  2\n",
            "Chosen Leaf:  3\n",
            "Optimal Hyperparameters Train Accuracy:  0.955\n",
            "Optimal Hyperparameters Validation Accuracy:  0.93\n",
            "Optimal Hyperparameters Test Accuracy:  0.8985507246376812\n",
            "Training Scores per Max Leaf Node Count: [0.945, 0.955, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]\n",
            "Validation Scores per Max Leaf Node Count: [0.91, 0.93, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEXCAYAAAC+mHPKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVDBERCQu/x84Tb5SqbQrUwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0-UXUer3SCd"
      },
      "source": [
        "### Question 5.3 Report on LaTeX\n",
        "Answer the followings question on LaTeX in the respective section.\n",
        "1. Report the values in your LaTeX document.\n",
        "2. How did the training accuracy and testing accuracy change after tuning compared to before? Briefly explain why.\n",
        "3. Paste the plot and explain any trends or patterns with the plot within validation and training scores and briefly explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G2xmO04DXCwF"
      },
      "source": [
        "# Question 6: Feature Scaling Effects on KNNs and DTs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NG7pZLBoXCwG"
      },
      "source": [
        "### Observing effects of standardizing features\n",
        "\n",
        "Up until now, we have not been using standardized features. Let's observe the effects of standardized features with decision trees and KNNs.\n",
        "\n",
        "Standardization, or feature scaling / data normalization, is a common preprocessing step for data within machine learning. We will see why it's important.\n",
        "\n",
        "Here is a definition taken from SK-Learn's website on Standardization:\n",
        "\n",
        "*Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.*\n",
        "\n",
        "*In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.*\n",
        "\n",
        "Learn More: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "\n",
        "To start, uncomment the code below and run to retrieve the data. (Recomment before submission.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T5RokryGXCwH",
        "colab": {}
      },
      "source": [
        "# # We will also use the same breast cancer classification dataset in Task 1.\n",
        "# # from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast = load_breast_cancer()\n",
        "\n",
        "X = breast['data']\n",
        "y = breast['target']\n",
        "\n",
        "np.random.seed(100)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "X_train, y_train = X[:400], y[:400]\n",
        "X_val, y_val = X[400:500], y[400:500]\n",
        "X_test, y_test = X[500:], y[500:]\n",
        "\n",
        "# Normalize data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9COC_Pa3SCg"
      },
      "source": [
        "### Helper Functions\n",
        "  \n",
        "We implemented above the KNN algorithm. Sci-kit learn also has their own version of the KNN algorithm which we will use in this following task. Use the two helper functions below in this next task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LJvoFpqh3SCg",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def train_KNN(X, y, norm=2, K=5):\n",
        "    \"\"\"\n",
        "    Trains a KNN classifier on the given X, y data with the specified \n",
        "    norm and K.\n",
        "\n",
        "    Args:\n",
        "        X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                              p (number of features) matrix\n",
        "        y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "        norm (int): The number form of the norm. Note that sklearn only allows L1 and L2 norms,\n",
        "                    (norm would be 1 and 2 respectively). Default is 2.\n",
        "        K (int): The value of K for the KNN algorithm. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        clf(KNeighborsClassifier): the trained KNN model\n",
        "    \"\"\"\n",
        "\n",
        "    clf = KNeighborsClassifier(n_neighbors=K, p=norm)\n",
        "    clf.fit(X,y)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def train_decision_tree(X, y, depth=None, leaf_count=None):\n",
        "    \"\"\"\n",
        "    This helper function is defined again from a previous section. \n",
        "\n",
        "    Trains a decision tree classifier on the given X, y data with the specified \n",
        "    tree depth d and max leaf node count max_leaf_num.\n",
        "\n",
        "    Args:\n",
        "        X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                              p (number of features) matrix\n",
        "        y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "        depth (int): The maximum depth of the tree. A value of None means no restrictions\n",
        "                 on the depth of the tree. Default is None.\n",
        "        leaf_count (int): The maximum leaf count of the tree's leaf nodes. A value of None means \n",
        "        no restrictions on the leaf count of the tree. Default is None.\n",
        "\n",
        "    Returns:\n",
        "        clf(DecisionTreeClassifier): the trained decision tree\n",
        "    \"\"\"\n",
        "\n",
        "    clf = DecisionTreeClassifier(max_depth=depth, max_leaf_nodes=leaf_count, criterion=\"entropy\", random_state=1)\n",
        "    clf.fit(X,y)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def predict(clf, X_test):\n",
        "    \"\"\"\n",
        "    This helper function is defined again from a previous section. \n",
        "\n",
        "    Uses a trained model to predict on a given test set.\n",
        "\n",
        "    Args:\n",
        "        clf (Classifier): Trained classifier such as KNN or Decision Tree\n",
        "        X_test ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                                   p (number of features) matrix\n",
        "\n",
        "    Returns:\n",
        "        y_pred ((n,) np.ndarray): The output predictions, which are in an n length array\n",
        "    \"\"\"\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "def evaluate(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    This helper function is defined again from a previous section. \n",
        "    \n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values: numpy array\n",
        "        actual_values: numpy array\n",
        "    \n",
        "    Returns:\n",
        "        a floating point number representing the accuracy\n",
        "    \"\"\"\n",
        "    return accuracy_score(predicted_values, actual_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iVfyIhUiXCwK"
      },
      "source": [
        "### Retrieving Metrics for Unstandardized Data\n",
        "Fill out this function to retrieve training and test accuracies for both KNN and decision tree models. Use default hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K0XxTvHMXCwM",
        "colab": {}
      },
      "source": [
        "def get_classifier_metrics(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Create a decision tree and KNN classifer on the normal dataset.\n",
        "    Retrieve accuracy metrics using models trained on default hyperparameters:\n",
        "\n",
        "    Args: (Note that n is not the same among train and test sets, \n",
        "         but merely refers to sample size)\n",
        "        X_train ((n,p) np.ndarray)\n",
        "        y_train ((n,) np.ndarray)\n",
        "        X_test ((n,p) np.ndarray)\n",
        "        y_test ((n,) np.ndarray)\n",
        "\n",
        "    Returns:\n",
        "        knn_train_accuracy (float): Accuracy of KNN for train set\n",
        "        knn_test_accuracy (float): Accuracy of KNN for test set\n",
        "        dt_train_accuracy (float): Accuracy of DT for train set\n",
        "        dt_test_accuracy (float): Accuracy of DT for test set    \n",
        "    \"\"\"\n",
        "\n",
        "    # <---- Your code here ----->\n",
        "    \n",
        "    # KNN model\n",
        "    clf_KNN = train_KNN(X_train, y_train)\n",
        "    pred_KNN_train = predict(clf_KNN, X_train)\n",
        "    pred_KNN_test = predict(clf_KNN, X_test)\n",
        "    knn_train_accuracy = evaluate(pred_KNN_train, y_train)\n",
        "    knn_test_accuracy = evaluate(pred_KNN_test, y_test)\n",
        "    \n",
        "    # decision tree model\n",
        "    clf_dt = train_decision_tree(X_train, y_train)\n",
        "    pred_dt_train = predict(clf_dt, X_train)\n",
        "    pred_dt_test = predict(clf_dt, X_test)\n",
        "    dt_train_accuracy = evaluate(pred_dt_train, y_train)\n",
        "    dt_test_accuracy = evaluate(pred_dt_test, y_test)\n",
        "\n",
        "    # <---- Your code here ----->\n",
        "\n",
        "    print(\"knn_train_accuracy: \", knn_train_accuracy)\n",
        "    print(\"knn_test_accuracy: \", knn_test_accuracy)\n",
        "    print(\"dt_train_accuracy: \", dt_train_accuracy)\n",
        "    print(\"dt_test_accuracy: \", dt_test_accuracy)\n",
        "\n",
        "    return knn_train_accuracy, knn_test_accuracy, dt_train_accuracy, dt_test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rF3Jmuy53SCk"
      },
      "source": [
        "Uncomment the code below and run the code (remember to recomment before submitting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kWS33o6OXCwO",
        "outputId": "6c1aab92-7fe1-45f7-cb13-c61bdd487a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(\"FOR UNSTANDARDIZED DATA\")\n",
        "get_classifier_metrics(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nFOR STANDARDIZED DATA\")\n",
        "get_classifier_metrics(X_train_scaled, y_train, X_test_scaled, y_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOR UNSTANDARDIZED DATA\n",
            "knn_train_accuracy:  0.9475\n",
            "knn_test_accuracy:  0.8985507246376812\n",
            "dt_train_accuracy:  1.0\n",
            "dt_test_accuracy:  0.9130434782608695\n",
            "\n",
            "FOR STANDARDIZED DATA\n",
            "knn_train_accuracy:  0.9775\n",
            "knn_test_accuracy:  0.9565217391304348\n",
            "dt_train_accuracy:  1.0\n",
            "dt_test_accuracy:  0.9130434782608695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9775, 0.9565217391304348, 1.0, 0.9130434782608695)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ZCmPf0G3SCn"
      },
      "source": [
        "### Question 6 Report in LaTeX\n",
        "Answer the following questions in your LaTeX document.\n",
        "1. Report the values in LaTeX.\n",
        "2. What happens to performance when we use standardization for data with decision trees? What about KNN? Briefly explain why each happened."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IjqhKS-oXCwR"
      },
      "source": [
        "# Turning it in\n",
        "\n",
        "\n",
        "**Remember to recomment all script portions of this notebook before submitting (i.e. any code not in a function, excluding code that imports libraries). This is to ensure that the Autograder works properly. Also make sure you did not edit other sections of the code outside of specified areas.**\n",
        "\n",
        "1. Download this notebook as a `hw1.py` file with the functions implemented and the sandbox code commented out\n",
        "  - If using Google Colab, go to \"File -> Download .py\"\n",
        "  - If using Jupyter locally, go to \"File -> Download as -> Python (.py)\"\n",
        "  \n",
        "2. Submit `hw1.py` file to Gradescope (you can do this as many times as you'd like before the deadline)"
      ]
    }
  ]
}