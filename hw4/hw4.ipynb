{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KAqXcDzKwj4D","colab_type":"text"},"source":["# Homework 4: Coding\n","\n","**Due Monday October 7th, 11:59pm.**\n","\n","**This is an individual assignment.**\n","\n","**There is no autograder submission, please add your responses and graphs to your LaTeX submission as indicated in the assignment.**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mqy1LtZCIQuG","colab":{}},"source":["\"\"\"\n","Import libraries that you might require\n","\"\"\"\n","\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import operator\n","from sklearn.metrics import accuracy_score\n","import sklearn.model_selection as ms\n","\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.neural_network import MLPClassifier\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5qmuIim2IQuK","outputId":"46f6e815-7194-4295-93f6-d721bccd524e","executionInfo":{"status":"ok","timestamp":1570209654076,"user_tz":240,"elapsed":470,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\"\n","Load data (MNIST digits dataset).\n","\n","Note that we will skip the validation phase for\n","this exercise as by now you are pretty familiar with the typical Machine Learning\n","pipeline.\n","\"\"\"\n","\n","from sklearn.datasets import load_digits\n","digits = load_digits()\n","print(digits.data.shape)\n","\n","X = digits['data']\n","y = digits['target']\n","\n","np.random.seed(100)\n","p = np.random.permutation(len(X))\n","X, y = X[p], y[p]\n","\n","X_train, y_train = X[:1500], y[:1500]\n","X_test, y_test = X[1500:], y[1500:]\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1797, 64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XmURULzoXCvq"},"source":["# Question 2: Performance Comparisons for three ML algorithms"]},{"cell_type":"markdown","metadata":{"id":"z_EG5SVkxrzs","colab_type":"text"},"source":["## Random Forest Classifier"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9KC2aNmoXCvr","outputId":"78bfc9cd-7e7c-4fd9-cbb0-2890c1ea6b59","executionInfo":{"status":"ok","timestamp":1570210231799,"user_tz":240,"elapsed":2339,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["class randomForestScikit:\n","  \n","  def modelRF(self):\n","    \"\"\"\n","    Creates model objects for the Random Forest Classifier.\n","    See the documentation in sklearn here:\n","    https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n","    \"\"\"\n","    \n","    list_n_estimators = [1, 10, 50, 100, 500]\n","    random_state = 42 # Do not change this random_state\n","\n","  \n","    objs_RFC = []\n","    \n","    # To complete: Create a list of objects for the classifier for each of the above \"n_estimators\"\n","    \n","    for ele_num_esti in list_n_estimators:\n","        ele_RF = RandomForestClassifier(n_estimators=ele_num_esti, random_state=random_state)\n","        objs_RFC.append(ele_RF)\n","\n","    return objs_RFC\n","    \n","    \n","  def trainRF(self, models, X_train, y_train, X_test, y_test):\n","    \"\"\"\n","    Trains several models and returns the test accuracy for each of them\n","    Args:\n","        models: list of model objects\n","    Returns:\n","        score (list): list of accuracies of the different fitted models on test set\n","    \"\"\"\n","    \n","    accuracies = []\n","\n","    # To complete: train and test each model in a for lop\n","    for ele_model in models:\n","        # train the model\n","        clf = ele_model.fit(X_train, y_train)\n","#         y_pred = clf.predict(X_test)\n","        score = clf.score(X_test, y_test)\n","        accuracies.append(score)\n","\n","\n","    return accuracies\n","\n","\n","# To complete: call the above class to train and test the Random Forest Classifier\n","RF_class = randomForestScikit()\n","list_RFC = RF_class.modelRF()\n","accuracies = RF_class.trainRF(list_RFC, X_train, y_train, X_test, y_test)\n","print('accuracies', accuracies)\n","\n","list_n_estimators = [1, 10, 50, 100, 500]\n","print(list_n_estimators)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["accuracies [0.7643097643097643, 0.9461279461279462, 0.9663299663299664, 0.9696969696969697, 0.9730639730639731]\n","[1, 10, 50, 100, 500]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oJrNWsSgz6yh","colab_type":"text"},"source":["## Kernel SVM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZLg9s5T4pAus","outputId":"63bfa97e-505e-4039-f237-c4a9e31b7d2b","executionInfo":{"status":"ok","timestamp":1570210026536,"user_tz":240,"elapsed":1114,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["\n","class kernelSVMScikit:\n","  \n","  def modelKSVM(self):\n","    \"\"\"\n","    Creates model objects for the Kernel SVM.\n","    See the documentation in sklearn here:\n","    https://scikit-learn.org/stable/modules/svm.html\n","    \"\"\"\n","    \n","    list_kernel_type = ['linear', 'poly', 'rbf']\n","    random_state = 42 # Do not change this random_state\n","\n","    objs_KSVM = []\n","    \n","    # To complete: Create a list of objects for the classifier for each of the above \"kernel\"\n","    \n","    for ele_kernel_type in list_kernel_type:\n","        ele_SVM = SVC(kernel=ele_kernel_type, random_state=random_state)\n","        objs_KSVM.append(ele_SVM)\n","\n","\n","    return objs_KSVM\n","    \n","    \n","  def trainKSVM(self, models, X_train, y_train, X_test, y_test):\n","    \"\"\"\n","    Trains several models and returns the test accuracy for each of them\n","    Args:\n","        models: list of model objects\n","    Returns:\n","        score (list): list of accuracies of the different fitted models on test set\n","    \"\"\"\n","\n","    accuracies = []\n","    \n","    # To complete: loop through the different models created\n","    \n","    for ele_model in models:\n","        # train the model\n","        clf = ele_model.fit(X_train, y_train)\n","#         y_pred = clf.predict(X_test)\n","        score = clf.score(X_test, y_test)\n","        accuracies.append(score)\n","\n","   \n","    return accuracies\n","\n","\n","# To complete: Call the above class to train and test the Random Forest Classifier\n","KSVM_class = kernelSVMScikit()\n","list_KSVM = KSVM_class.modelKSVM()\n","accuracies = KSVM_class.trainKSVM(list_KSVM, X_train, y_train, X_test, y_test)\n","print('accuracies', accuracies)\n","\n","list_kernel_type = ['linear', 'poly', 'rbf']\n","print(list_kernel_type)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["accuracies [0.9831649831649831, 0.9865319865319865, 0.4478114478114478]\n","['linear', 'poly', 'rbf']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8vSc15PpxyS2","colab_type":"text"},"source":["## Multi Layer Perceptron"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ejmbeisHpBdO","outputId":"c8927fc4-b5ef-4145-ed93-173e47abf469","executionInfo":{"status":"ok","timestamp":1570210140795,"user_tz":240,"elapsed":9729,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["\n","class MLPScikit:\n","  \n","  def modelMLP(self):\n","    \"\"\"\n","    Creates model objects for the Multi Layered Perceptron.\n","    See the documentation in sklearn here:\n","    https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n","    \"\"\"\n","    \n","    \n","    layerSizes = [(3), (10), (10,10,10), (20,50,20)]\n","    random_state = 42 # Do not change this random_state\n","    max_iter = 2000 # fixed max_iter\n","    \n","    objs_MLP = []\n","\n","    # To complete: Create a list of objects for the classifier for each of the above \"n_estimators\"\n","    \n","    for ele_size in layerSizes:\n","        ele_MLP = MLPClassifier(hidden_layer_sizes=ele_size, max_iter=max_iter, random_state=random_state)\n","        objs_MLP.append(ele_MLP)\n","\n","    return objs_MLP\n","    \n","    \n","  def trainMLP(self, models, X_train, y_train, X_test, y_test):\n","    \"\"\"\n","    Trains several models and returns the test accuracy for each of them\n","    Args:\n","        models: list of model objects\n","    Returns:\n","        score (list): list of accuracies of the different fitted models on test set\n","    \"\"\"\n","    \n","    accuracies = []\n","\n","    # To complete: train and test each model in a for lop\n","    \n","    for ele_model in models:\n","        # train the model\n","        clf = ele_model.fit(X_train, y_train)\n","#         y_pred = clf.predict(X_test)\n","        score = clf.score(X_test, y_test)\n","        accuracies.append(score)\n","\n","\n","    return accuracies\n","\n","\n","# To complete: Call the above class to train and test the Multi Layer Perceptron\n","MLP_class = MLPScikit()\n","list_MLP = MLP_class.modelMLP()\n","accuracies = MLP_class.trainMLP(list_MLP, X_train, y_train, X_test, y_test)\n","print('accuracies', accuracies)\n","\n","layerSizes = [(3), (10), (10,10,10), (20,50,20)]\n","print(layerSizes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["accuracies [0.7205387205387206, 0.9427609427609428, 0.9461279461279462, 0.9629629629629629]\n","[3, 10, (10, 10, 10), (20, 50, 20)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U9fIU8eP_X1u","colab_type":"text"},"source":["# Question 3.2 Convolutional Neural Networks  \n","In this assignment you will be training a Convolutional Neural Network on  \n","the Fashion MNIST dataset.  \n","\n","You may find more information about the dataset [here](https://github.com/zalandoresearch/fashion-mnist).  \n","For this assignment we have already loaded the dataset for you.  \n","  \n","You will be using PyTorch for implementing your CNN. \n","\n","**We highly recommend following [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) for this question** as well as referring to the [official documentation](https://pytorch.org/docs/stable/nn.html) if you are unfamiliar with Pytorch."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mL1q9V0O13B6"},"source":["## Setup: Load Tensorboard\n","\n","The below code is used to load [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard), which is used to visualize the training and execution of your neural network.\n","\n","Run the below cells and click on the Tensorboard link produced by the third cell below while your network is training (Section 3.2.5) to plot the accuracy and loss curves."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XWS884Qt13Bd","colab":{}},"source":["!rm -r -f ./logs\n","\n","LOG_DIR = './logs'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rvbnO1JX13BO","colab":{}},"source":["get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"w0lafkzw13Ak","outputId":"07a1c2a8-4cd0-4e50-f52e-8abc474dae94","executionInfo":{"status":"ok","timestamp":1570503015976,"user_tz":240,"elapsed":7163,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensorboard Link: http://a6331d24.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HSeELIRYjGSG","colab_type":"text"},"source":["## Setup: Logger\n","\n","Please look at the functions the logger class provides. You may use them to log   \n","training metrics like loss, accuracy and even some selected images and their  \n","labels to see how network parameters change during training."]},{"cell_type":"code","metadata":{"id":"ipIykcBHt1dk","colab_type":"code","colab":{}},"source":["# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n","import tensorflow as tf\n","import numpy as np\n","import scipy.misc \n","try:\n","    from StringIO import StringIO  # Python 2.7\n","except ImportError:\n","    from io import BytesIO         # Python 3.x\n","\n","\n","class Logger(object):\n","    \n","    def __init__(self, log_dir):\n","        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n","        self.writer = tf.summary.FileWriter(log_dir)\n","\n","    def scalar_summary(self, tag, value, step):\n","        \"\"\"Log a scalar variable.\"\"\"\n","        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n","        self.writer.add_summary(summary, step)\n","\n","    def image_summary(self, tag, images, step):\n","        \"\"\"Log a list of images.\"\"\"\n","\n","        img_summaries = []\n","        for i, img in enumerate(images):\n","            # Write the image to a string\n","            try:\n","                s = StringIO()\n","            except:\n","                s = BytesIO()\n","            scipy.misc.toimage(img).save(s, format=\"png\")\n","\n","            # Create an Image object\n","            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n","                                       height=img.shape[0],\n","                                       width=img.shape[1])\n","            # Create a Summary value\n","            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n","\n","        # Create and write Summary\n","        summary = tf.Summary(value=img_summaries)\n","        self.writer.add_summary(summary, step)\n","        \n","    def histo_summary(self, tag, values, step, bins=1000):\n","        \"\"\"Log a histogram of the tensor of values.\"\"\"\n","\n","        # Create a histogram using numpy\n","        counts, bin_edges = np.histogram(values, bins=bins)\n","\n","        # Fill the fields of the histogram proto\n","        hist = tf.HistogramProto()\n","        hist.min = float(np.min(values))\n","        hist.max = float(np.max(values))\n","        hist.num = int(np.prod(values.shape))\n","        hist.sum = float(np.sum(values))\n","        hist.sum_squares = float(np.sum(values**2))\n","\n","        # Drop the start of the first bin\n","        bin_edges = bin_edges[1:]\n","\n","        # Add bin edges and counts\n","        for edge in bin_edges:\n","            hist.bucket_limit.append(edge)\n","        for c in counts:\n","            hist.bucket.append(c)\n","\n","        # Create and write Summary\n","        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n","        self.writer.add_summary(summary, step)\n","        self.writer.flush()\n","logger = Logger('./logs')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIkuagd9_X1v","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","from torchvision import datasets, transforms"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qvDAZByo_X1w","colab_type":"text"},"source":["##3.2.1 Loading the Dataset\n","The output of torchvision datasets are PILImage images of range [0, 1].  \n","We transform them to Tensors of normalized range [-1, 1].  \n","```Transforms.Normalize((mean,),(std,))``` basically manipulates the values of a pixel such that  \n","$$New\\_Value = \\frac{Old\\_Value - Mean}{Std}$$\n","\n"]},{"cell_type":"code","metadata":{"id":"lK7aviwP_X1x","colab_type":"code","outputId":"ee6c4a11-1180-4b73-f48b-ddabcdc92031","executionInfo":{"status":"ok","timestamp":1570503023293,"user_tz":240,"elapsed":14462,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["# Define a transform to normalize the data\n","\n","#TODO : Set the value of mean and the standard deviation to \n","#       normalize the image from range [0,1] to the range [-1, 1]\n","\n","\n","#Begin Your Code\n","\n","mean = 0.5\n","std = 0.5\n","\n","#End Your Code\n","\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((mean,), (std,))\n","                                ])\n","\n","\n","#TODO : Select suitable value of batch_sizes.\n","\n","#Begin Your Code\n","\n","train_batch_size = 16\n","test_batch_size = 16\n","\n","#End Your Code\n","\n","# Download and load the training data\n","trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n","\n","# Download and load the test data\n","testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=True)\n","\n","\n","# Classes\n","classes = {       0 :'T-shirt/top',\n","                  1 :'Trouser',\n","                  2 :'Pullover',\n","                  3 :'Dress',\n","                  4 :'Coat',\n","                  5 :'Sandal',\n","                  6 :'Shirt',\n","                  7 :'Sneaker',\n","                  8 :'Bag',\n","                  9 :'Ankle boot'}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  0%|          | 0/26421880 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["26427392it [00:00, 68078868.51it/s]                              \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 478359.68it/s]\n","  0%|          | 16384/4422102 [00:00<00:27, 158512.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["4423680it [00:00, 20191375.44it/s]                           \n","8192it [00:00, 187921.41it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n","Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SFBh6ZhO_X1y","colab_type":"text"},"source":["##3.2.2 The Dataset\n","Here we show some images of the dataset.  \n","See how many of the categories can you recognise.\n","\n"]},{"cell_type":"code","metadata":{"id":"hX5Irrw-_X1z","colab_type":"code","outputId":"bafc9e5a-873a-42d3-d286-bfe5d8b7f56e","executionInfo":{"status":"ok","timestamp":1570503024500,"user_tz":240,"elapsed":15661,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":325}},"source":["import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","import numpy as np\n","\n","# Functions to show an image\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    \n","    figure(num=None, figsize=(8, 6), dpi=150, edgecolor='k')\n","    plt.axis('off')\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","\n","# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA+kAAAE0CAYAAABQAnKnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<Figure size 1200x900 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"le9N91sO_X11","colab_type":"text"},"source":["##3.2.3 Create your Convolutional Neural Network\n","Create the CNN with layers as mentioned in the LaTeX pdf for full credit.  \n","You are, however, free to change the architecture as long as you achieve accuracy better than the architecture shown in the LaTeX pdf.\n","\n"]},{"cell_type":"code","metadata":{"id":"dWmqB-1b_X11","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        #TODO : Design your network, you are allowed to explore your own architecture\n","        #       But you should achieve a better overall accuracy than the baseline network.\n","        #       Also, if you do design your own network, include an explanation \n","        #       for your choice of network and how it may be better than the \n","        #       baseline network in your latex.\n","        \n","        #Begin Your Code\n","        \n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(5,5), stride=1, padding=2)\n","        self.conv2 = nn.Conv2d(16, 32, (5,5), stride=1, padding=2)\n","        \n","        self.bn1 = nn.BatchNorm2d(1)\n","        self.bn2 = nn.BatchNorm2d(16)\n","        \n","        self.maxpooling = nn.MaxPool2d(2)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.linear = nn.Linear(32*7*7, 10)\n","#         self.linear = nn.Linear()\n","        self.softmax = nn.Softmax()\n","        self.flatten = nn.Flatten()\n","        \n","        self.res = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(22,22), stride=1, padding=0),\n","            nn.BatchNorm2d(32)\n","            )\n","\n","        #End Your Code\n","\n","    def forward(self, x):\n","\n","      #TODO : Implement the forward function that applies the layers you have created to the input\n","\n","      #Begin Your Code\n","      \n","      x = self.conv1(x)\n","      x = self.relu(x)\n","      x = self.maxpooling(x)\n","      # ------\n","      x = self.conv2(x)\n","      x = self.relu(x)\n","      x = self.maxpooling(x)  # [16,32,7,7]\n","#       print('x.SHAPE', x.shape)\n","        \n","    \n","    \n","    \n","#       residual = self.res(x)\n","      \n","#       x = self.bn1(x)\n","#       x = self.relu(x)\n","#       x = self.conv1(x)\n","#       x = self.maxpooling(x)\n","#       # ------\n","#       x = self.bn2(x)\n","#       x = self.relu(x)\n","#       x = self.conv2(x)\n","#       x = self.maxpooling(x)\n","      \n","#       x += residual\n","        \n","      # ------\n","      out = self.flatten(x)\n","      out = F.dropout(out, p=0.5, training=True, inplace=True)  # [32,32,7,7] [C,H,W]\n","#       print('out.SHAPE', out.shape)\n","      out = self.linear(out)\n","#       out = self.softmax(out)\n","      \n","#       print('out.SHAPE', out.shape)\n","      \n","      return out\n","\n","      #End Your Code\n","\n","\n","net = Net()\n","# print(net)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jieF6xy__X13","colab_type":"text"},"source":["##3.2.4 Define a Loss function and optimizer\n","We will be using [Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) and [Adam optimizer](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam).  \n","Note: PyTorch's CrossEntropyLoss combines log softmax and negative log likelihood loss in one class. Make sure you are not computing softmax twice.\n","\n"]},{"cell_type":"code","metadata":{"id":"_5-Qn1ef_X13","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","#TODO : Use appropriate loss criterion and optimizer \n","\n","#Begin Your Code\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","\n","#End Your Code"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D4za3t0b_X15","colab_type":"text"},"source":["##3.2.5. Train the network\n","\n","Here we are going to train the network while logging the per batch metrics.  \n","This would take some time to run (5-10 minutes).\n","\n"]},{"cell_type":"code","metadata":{"id":"r7y1qZI1wyvv","colab_type":"code","outputId":"e80bca7f-5713-4c99-c57f-785c8d09057f","executionInfo":{"status":"ok","timestamp":1570504416650,"user_tz":240,"elapsed":1407787,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["overall_step = 0\n","\n","#TODO : Select appropriate number of epochs\n","\n","#Begin Your Code\n","\n","epochs = 30\n","\n","#End Your Code\n","\n","acc_up = 0\n","acc_down = 0\n","\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","    running_loss = 0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data  # input [32,1,28,28], labels [32], b=32\n","#         print('labels.SHAPE', labels.shape)\n","#         print('inputs.SHAPE', inputs.shape)\n","\n","        #TODO : Make predictions, calculate accuracy and update your weights once\n","\n","        #Begin Your Code\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        \n","        # get prediction\n","        pred = net(inputs)  # [32,10]\n","#         print('pred.SHAPE', pred.shape)\n","        # get loss\n","        loss = criterion(pred, labels)\n","        # calculate accuracy\n","        pred_result = pred.argmax(dim=1)\n","#         print('pred_result', pred_result)\n","#         print('labels', labels)\n","        acc_up += pred_result.eq(labels).float().sum()\n","#         print('acc_up', acc_up)\n","        acc_down += train_batch_size\n","#         print('acc_down', acc_down)\n","        accuracy = acc_up / acc_down\n","#         print('acc', accuracy)\n","        \n","        # back & step\n","        loss.backward()\n","        optimizer.step()\n","        \n","\n","        #End Your Code\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 200 == 199:    # print every 200 mini-batches\n","            print('Epoch: %d, Batch: %5d, loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 200))\n","            print('Accuracy', accuracy.item())\n","            running_loss = 0.0\n","            #Any thing that is added to the \"info\" gets plotted in tensorboard\n","            #TODO : Add the plots in Tensorboard to the report and explain what is happening\n","            info = { ('loss') : loss.item(),('accuracy'): accuracy.item()}\n","            for tag, value in info.items():\n","                logger.scalar_summary(tag, value, overall_step+1)\n","                overall_step += 1 # update step num\n","\n","print('Finished Training')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1, Batch:   200, loss: 0.960\n","Accuracy 0.6578124761581421\n","Epoch: 1, Batch:   400, loss: 0.613\n","Accuracy 0.715624988079071\n","Epoch: 1, Batch:   600, loss: 0.545\n","Accuracy 0.7443749904632568\n","Epoch: 1, Batch:   800, loss: 0.514\n","Accuracy 0.7602343559265137\n","Epoch: 1, Batch:  1000, loss: 0.507\n","Accuracy 0.7731249928474426\n","Epoch: 1, Batch:  1200, loss: 0.471\n","Accuracy 0.7823437452316284\n","Epoch: 1, Batch:  1400, loss: 0.458\n","Accuracy 0.7895535826683044\n","Epoch: 1, Batch:  1600, loss: 0.453\n","Accuracy 0.7967968583106995\n","Epoch: 1, Batch:  1800, loss: 0.426\n","Accuracy 0.8023611307144165\n","Epoch: 1, Batch:  2000, loss: 0.410\n","Accuracy 0.8072500228881836\n","Epoch: 1, Batch:  2200, loss: 0.427\n","Accuracy 0.8113352060317993\n","Epoch: 1, Batch:  2400, loss: 0.394\n","Accuracy 0.8147395849227905\n","Epoch: 1, Batch:  2600, loss: 0.391\n","Accuracy 0.818461537361145\n","Epoch: 1, Batch:  2800, loss: 0.387\n","Accuracy 0.8215401768684387\n","Epoch: 1, Batch:  3000, loss: 0.406\n","Accuracy 0.8240000009536743\n","Epoch: 1, Batch:  3200, loss: 0.380\n","Accuracy 0.8266991972923279\n","Epoch: 1, Batch:  3400, loss: 0.393\n","Accuracy 0.8291544318199158\n","Epoch: 1, Batch:  3600, loss: 0.376\n","Accuracy 0.8316145539283752\n","Epoch: 2, Batch:   200, loss: 0.354\n","Accuracy 0.8346044421195984\n","Epoch: 2, Batch:   400, loss: 0.339\n","Accuracy 0.8370632529258728\n","Epoch: 2, Batch:   600, loss: 0.359\n","Accuracy 0.8384913802146912\n","Epoch: 2, Batch:   800, loss: 0.369\n","Accuracy 0.8396291136741638\n","Epoch: 2, Batch:  1000, loss: 0.329\n","Accuracy 0.8413158059120178\n","Epoch: 2, Batch:  1200, loss: 0.352\n","Accuracy 0.8424873948097229\n","Epoch: 2, Batch:  1400, loss: 0.367\n","Accuracy 0.8433859348297119\n","Epoch: 2, Batch:  1600, loss: 0.346\n","Accuracy 0.8446028232574463\n","Epoch: 2, Batch:  1800, loss: 0.364\n","Accuracy 0.845168948173523\n","Epoch: 2, Batch:  2000, loss: 0.367\n","Accuracy 0.8461086750030518\n","Epoch: 2, Batch:  2200, loss: 0.314\n","Accuracy 0.8474684953689575\n","Epoch: 2, Batch:  2400, loss: 0.332\n","Accuracy 0.8485569357872009\n","Epoch: 2, Batch:  2600, loss: 0.341\n","Accuracy 0.8495275378227234\n","Epoch: 2, Batch:  2800, loss: 0.346\n","Accuracy 0.8503721356391907\n","Epoch: 2, Batch:  3000, loss: 0.363\n","Accuracy 0.8510555624961853\n","Epoch: 2, Batch:  3200, loss: 0.356\n","Accuracy 0.8517445921897888\n","Epoch: 2, Batch:  3400, loss: 0.360\n","Accuracy 0.8520541787147522\n","Epoch: 2, Batch:  3600, loss: 0.320\n","Accuracy 0.8528486490249634\n","Epoch: 3, Batch:   200, loss: 0.310\n","Accuracy 0.8540665507316589\n","Epoch: 3, Batch:   400, loss: 0.317\n","Accuracy 0.8549050688743591\n","Epoch: 3, Batch:   600, loss: 0.333\n","Accuracy 0.85544753074646\n","Epoch: 3, Batch:   800, loss: 0.306\n","Accuracy 0.8562273979187012\n","Epoch: 3, Batch:  1000, loss: 0.310\n","Accuracy 0.856955885887146\n","Epoch: 3, Batch:  1200, loss: 0.321\n","Accuracy 0.8575502634048462\n","Epoch: 3, Batch:  1400, loss: 0.319\n","Accuracy 0.8582303524017334\n","Epoch: 3, Batch:  1600, loss: 0.302\n","Accuracy 0.8590178489685059\n","Epoch: 3, Batch:  1800, loss: 0.323\n","Accuracy 0.8594892621040344\n","Epoch: 3, Batch:  2000, loss: 0.336\n","Accuracy 0.8599802851676941\n","Epoch: 3, Batch:  2200, loss: 0.327\n","Accuracy 0.8604574799537659\n","Epoch: 3, Batch:  2400, loss: 0.300\n","Accuracy 0.8610795736312866\n","Epoch: 3, Batch:  2600, loss: 0.322\n","Accuracy 0.8615284562110901\n","Epoch: 3, Batch:  2800, loss: 0.331\n","Accuracy 0.8619235157966614\n","Epoch: 3, Batch:  3000, loss: 0.328\n","Accuracy 0.8622321486473083\n","Epoch: 3, Batch:  3200, loss: 0.311\n","Accuracy 0.8626752495765686\n","Epoch: 3, Batch:  3400, loss: 0.297\n","Accuracy 0.8630848526954651\n","Epoch: 3, Batch:  3600, loss: 0.311\n","Accuracy 0.8635585308074951\n","Epoch: 4, Batch:   200, loss: 0.287\n","Accuracy 0.8644868731498718\n","Epoch: 4, Batch:   400, loss: 0.278\n","Accuracy 0.8651448488235474\n","Epoch: 4, Batch:   600, loss: 0.318\n","Accuracy 0.8656381964683533\n","Epoch: 4, Batch:   800, loss: 0.271\n","Accuracy 0.8662915229797363\n","Epoch: 4, Batch:  1000, loss: 0.298\n","Accuracy 0.8667193651199341\n","Epoch: 4, Batch:  1200, loss: 0.313\n","Accuracy 0.8669829368591309\n","Epoch: 4, Batch:  1400, loss: 0.305\n","Accuracy 0.8674308061599731\n","Epoch: 4, Batch:  1600, loss: 0.321\n","Accuracy 0.8676799535751343\n","Epoch: 4, Batch:  1800, loss: 0.308\n","Accuracy 0.8680124282836914\n","Epoch: 4, Batch:  2000, loss: 0.313\n","Accuracy 0.868306577205658\n","Epoch: 4, Batch:  2200, loss: 0.282\n","Accuracy 0.8687267899513245\n","Epoch: 4, Batch:  2400, loss: 0.305\n","Accuracy 0.8690292835235596\n","Epoch: 4, Batch:  2600, loss: 0.313\n","Accuracy 0.8692960143089294\n","Epoch: 4, Batch:  2800, loss: 0.299\n","Accuracy 0.8695595860481262\n","Epoch: 4, Batch:  3000, loss: 0.316\n","Accuracy 0.8697807192802429\n","Epoch: 4, Batch:  3200, loss: 0.310\n","Accuracy 0.8699610829353333\n","Epoch: 4, Batch:  3400, loss: 0.298\n","Accuracy 0.8702133297920227\n","Epoch: 4, Batch:  3600, loss: 0.262\n","Accuracy 0.870585024356842\n","Epoch: 5, Batch:   200, loss: 0.292\n","Accuracy 0.8711225390434265\n","Epoch: 5, Batch:   400, loss: 0.298\n","Accuracy 0.8713961243629456\n","Epoch: 5, Batch:   600, loss: 0.320\n","Accuracy 0.8715464472770691\n","Epoch: 5, Batch:   800, loss: 0.276\n","Accuracy 0.8719501495361328\n","Epoch: 5, Batch:  1000, loss: 0.277\n","Accuracy 0.8722422122955322\n","Epoch: 5, Batch:  1200, loss: 0.283\n","Accuracy 0.8725385665893555\n","Epoch: 5, Batch:  1400, loss: 0.271\n","Accuracy 0.8728277683258057\n","Epoch: 5, Batch:  1600, loss: 0.296\n","Accuracy 0.8729857206344604\n","Epoch: 5, Batch:  1800, loss: 0.284\n","Accuracy 0.8732961416244507\n","Epoch: 5, Batch:  2000, loss: 0.282\n","Accuracy 0.8735918998718262\n","Epoch: 5, Batch:  2200, loss: 0.297\n","Accuracy 0.8737754225730896\n","Epoch: 5, Batch:  2400, loss: 0.290\n","Accuracy 0.8740840554237366\n","Epoch: 5, Batch:  2600, loss: 0.308\n","Accuracy 0.8742897510528564\n","Epoch: 5, Batch:  2800, loss: 0.297\n","Accuracy 0.8744768500328064\n","Epoch: 5, Batch:  3000, loss: 0.272\n","Accuracy 0.8748124837875366\n","Epoch: 5, Batch:  3200, loss: 0.289\n","Accuracy 0.8750274777412415\n","Epoch: 5, Batch:  3400, loss: 0.304\n","Accuracy 0.8752275705337524\n","Epoch: 5, Batch:  3600, loss: 0.277\n","Accuracy 0.8755443692207336\n","Epoch: 6, Batch:   200, loss: 0.274\n","Accuracy 0.8759234547615051\n","Epoch: 6, Batch:   400, loss: 0.267\n","Accuracy 0.8761520981788635\n","Epoch: 6, Batch:   600, loss: 0.242\n","Accuracy 0.8764922618865967\n","Epoch: 6, Batch:   800, loss: 0.273\n","Accuracy 0.8767359256744385\n","Epoch: 6, Batch:  1000, loss: 0.271\n","Accuracy 0.8769778609275818\n","Epoch: 6, Batch:  1200, loss: 0.285\n","Accuracy 0.8771428465843201\n","Epoch: 6, Batch:  1400, loss: 0.278\n","Accuracy 0.8773635029792786\n","Epoch: 6, Batch:  1600, loss: 0.268\n","Accuracy 0.877570629119873\n","Epoch: 6, Batch:  1800, loss: 0.289\n","Accuracy 0.8777585029602051\n","Epoch: 6, Batch:  2000, loss: 0.283\n","Accuracy 0.8779457807540894\n","Epoch: 6, Batch:  2200, loss: 0.303\n","Accuracy 0.8780936598777771\n","Epoch: 6, Batch:  2400, loss: 0.285\n","Accuracy 0.8782358169555664\n","Epoch: 6, Batch:  2600, loss: 0.269\n","Accuracy 0.8784045577049255\n","Epoch: 6, Batch:  2800, loss: 0.285\n","Accuracy 0.87859046459198\n","Epoch: 6, Batch:  3000, loss: 0.260\n","Accuracy 0.8788304328918457\n","Epoch: 6, Batch:  3200, loss: 0.290\n","Accuracy 0.8789692521095276\n","Epoch: 6, Batch:  3400, loss: 0.272\n","Accuracy 0.8791506886482239\n","Epoch: 6, Batch:  3600, loss: 0.294\n","Accuracy 0.8792980909347534\n","Epoch: 7, Batch:   200, loss: 0.265\n","Accuracy 0.8795952796936035\n","Epoch: 7, Batch:   400, loss: 0.273\n","Accuracy 0.8797871470451355\n","Epoch: 7, Batch:   600, loss: 0.278\n","Accuracy 0.8799594044685364\n","Epoch: 7, Batch:   800, loss: 0.268\n","Accuracy 0.8801475167274475\n","Epoch: 7, Batch:  1000, loss: 0.252\n","Accuracy 0.8803989291191101\n","Epoch: 7, Batch:  1200, loss: 0.264\n","Accuracy 0.8806144595146179\n","Epoch: 7, Batch:  1400, loss: 0.259\n","Accuracy 0.8808368444442749\n","Epoch: 7, Batch:  1600, loss: 0.263\n","Accuracy 0.8810710310935974\n","Epoch: 7, Batch:  1800, loss: 0.271\n","Accuracy 0.8812268376350403\n","Epoch: 7, Batch:  2000, loss: 0.287\n","Accuracy 0.8813571333885193\n","Epoch: 7, Batch:  2200, loss: 0.264\n","Accuracy 0.8815283179283142\n","Epoch: 7, Batch:  2400, loss: 0.288\n","Accuracy 0.8816415667533875\n","Epoch: 7, Batch:  2600, loss: 0.285\n","Accuracy 0.8817305564880371\n","Epoch: 7, Batch:  2800, loss: 0.295\n","Accuracy 0.881833016872406\n","Epoch: 7, Batch:  3000, loss: 0.262\n","Accuracy 0.881977915763855\n","Epoch: 7, Batch:  3200, loss: 0.276\n","Accuracy 0.8821424841880798\n","Epoch: 7, Batch:  3400, loss: 0.258\n","Accuracy 0.8823528289794922\n","Epoch: 7, Batch:  3600, loss: 0.293\n","Accuracy 0.882452130317688\n","Epoch: 8, Batch:   200, loss: 0.254\n","Accuracy 0.8827386498451233\n","Epoch: 8, Batch:   400, loss: 0.270\n","Accuracy 0.8829221129417419\n","Epoch: 8, Batch:   600, loss: 0.261\n","Accuracy 0.8830912709236145\n","Epoch: 8, Batch:   800, loss: 0.249\n","Accuracy 0.8832855820655823\n","Epoch: 8, Batch:  1000, loss: 0.245\n","Accuracy 0.883490800857544\n","Epoch: 8, Batch:  1200, loss: 0.259\n","Accuracy 0.8837181329727173\n","Epoch: 8, Batch:  1400, loss: 0.259\n","Accuracy 0.8838675618171692\n","Epoch: 8, Batch:  1600, loss: 0.262\n","Accuracy 0.8839699029922485\n","Epoch: 8, Batch:  1800, loss: 0.256\n","Accuracy 0.8841443657875061\n","Epoch: 8, Batch:  2000, loss: 0.262\n","Accuracy 0.8842964768409729\n","Epoch: 8, Batch:  2200, loss: 0.278\n","Accuracy 0.884446382522583\n","Epoch: 8, Batch:  2400, loss: 0.265\n","Accuracy 0.8845288157463074\n","Epoch: 8, Batch:  2600, loss: 0.260\n","Accuracy 0.8846274018287659\n","Epoch: 8, Batch:  2800, loss: 0.275\n","Accuracy 0.8847310543060303\n","Epoch: 8, Batch:  3000, loss: 0.286\n","Accuracy 0.88482266664505\n","Epoch: 8, Batch:  3200, loss: 0.269\n","Accuracy 0.8849341869354248\n","Epoch: 8, Batch:  3400, loss: 0.275\n","Accuracy 0.8850189447402954\n","Epoch: 8, Batch:  3600, loss: 0.277\n","Accuracy 0.885136067867279\n","Epoch: 9, Batch:   200, loss: 0.259\n","Accuracy 0.8853849172592163\n","Epoch: 9, Batch:   400, loss: 0.254\n","Accuracy 0.8855407238006592\n","Epoch: 9, Batch:   600, loss: 0.250\n","Accuracy 0.8856679201126099\n","Epoch: 9, Batch:   800, loss: 0.266\n","Accuracy 0.8857975006103516\n","Epoch: 9, Batch:  1000, loss: 0.266\n","Accuracy 0.8858931660652161\n","Epoch: 9, Batch:  1200, loss: 0.252\n","Accuracy 0.8860276341438293\n","Epoch: 9, Batch:  1400, loss: 0.268\n","Accuracy 0.8860907554626465\n","Epoch: 9, Batch:  1600, loss: 0.276\n","Accuracy 0.8861313462257385\n","Epoch: 9, Batch:  1800, loss: 0.275\n","Accuracy 0.8862067461013794\n","Epoch: 9, Batch:  2000, loss: 0.233\n","Accuracy 0.8864101767539978\n","Epoch: 9, Batch:  2200, loss: 0.262\n","Accuracy 0.8865411281585693\n","Epoch: 9, Batch:  2400, loss: 0.261\n","Accuracy 0.8866531848907471\n","Epoch: 9, Batch:  2600, loss: 0.248\n","Accuracy 0.8867849111557007\n","Epoch: 9, Batch:  2800, loss: 0.263\n","Accuracy 0.8868921399116516\n","Epoch: 9, Batch:  3000, loss: 0.280\n","Accuracy 0.8869583606719971\n","Epoch: 9, Batch:  3200, loss: 0.249\n","Accuracy 0.8870915174484253\n","Epoch: 9, Batch:  3400, loss: 0.278\n","Accuracy 0.8871931433677673\n","Epoch: 9, Batch:  3600, loss: 0.242\n","Accuracy 0.8873642086982727\n","Epoch: 10, Batch:   200, loss: 0.246\n","Accuracy 0.8875865340232849\n","Epoch: 10, Batch:   400, loss: 0.258\n","Accuracy 0.887712299823761\n","Epoch: 10, Batch:   600, loss: 0.234\n","Accuracy 0.8878712058067322\n","Epoch: 10, Batch:   800, loss: 0.265\n","Accuracy 0.8880010843276978\n","Epoch: 10, Batch:  1000, loss: 0.264\n","Accuracy 0.8880755305290222\n","Epoch: 10, Batch:  1200, loss: 0.254\n","Accuracy 0.8881741762161255\n","Epoch: 10, Batch:  1400, loss: 0.237\n","Accuracy 0.888319730758667\n","Epoch: 10, Batch:  1600, loss: 0.270\n","Accuracy 0.8883716464042664\n","Epoch: 10, Batch:  1800, loss: 0.243\n","Accuracy 0.8884880542755127\n","Epoch: 10, Batch:  2000, loss: 0.254\n","Accuracy 0.888590931892395\n","Epoch: 10, Batch:  2200, loss: 0.270\n","Accuracy 0.8886630535125732\n","Epoch: 10, Batch:  2400, loss: 0.271\n","Accuracy 0.8887119889259338\n","Epoch: 10, Batch:  2600, loss: 0.260\n","Accuracy 0.8888118863105774\n","Epoch: 10, Batch:  2800, loss: 0.266\n","Accuracy 0.8888902068138123\n","Epoch: 10, Batch:  3000, loss: 0.270\n","Accuracy 0.8889591693878174\n","Epoch: 10, Batch:  3200, loss: 0.263\n","Accuracy 0.8890460133552551\n","Epoch: 10, Batch:  3400, loss: 0.259\n","Accuracy 0.8891335725784302\n","Epoch: 10, Batch:  3600, loss: 0.258\n","Accuracy 0.8892084956169128\n","Epoch: 11, Batch:   200, loss: 0.261\n","Accuracy 0.8893136382102966\n","Epoch: 11, Batch:   400, loss: 0.265\n","Accuracy 0.8893766403198242\n","Epoch: 11, Batch:   600, loss: 0.219\n","Accuracy 0.8895423412322998\n","Epoch: 11, Batch:   800, loss: 0.248\n","Accuracy 0.8896263241767883\n","Epoch: 11, Batch:  1000, loss: 0.243\n","Accuracy 0.8897451162338257\n","Epoch: 11, Batch:  1200, loss: 0.243\n","Accuracy 0.8898578882217407\n","Epoch: 11, Batch:  1400, loss: 0.242\n","Accuracy 0.889975905418396\n","Epoch: 11, Batch:  1600, loss: 0.237\n","Accuracy 0.8900735378265381\n","Epoch: 11, Batch:  1800, loss: 0.258\n","Accuracy 0.8901208639144897\n","Epoch: 11, Batch:  2000, loss: 0.260\n","Accuracy 0.8902136087417603\n","Epoch: 11, Batch:  2200, loss: 0.243\n","Accuracy 0.8903164267539978\n","Epoch: 11, Batch:  2400, loss: 0.257\n","Accuracy 0.8903947472572327\n","Epoch: 11, Batch:  2600, loss: 0.267\n","Accuracy 0.8904691338539124\n","Epoch: 11, Batch:  2800, loss: 0.276\n","Accuracy 0.8905304074287415\n","Epoch: 11, Batch:  3000, loss: 0.267\n","Accuracy 0.8905786871910095\n","Epoch: 11, Batch:  3200, loss: 0.272\n","Accuracy 0.8906418681144714\n","Epoch: 11, Batch:  3400, loss: 0.259\n","Accuracy 0.8907029628753662\n","Epoch: 11, Batch:  3600, loss: 0.248\n","Accuracy 0.8908029198646545\n","Epoch: 12, Batch:   200, loss: 0.230\n","Accuracy 0.8909695148468018\n","Epoch: 12, Batch:   400, loss: 0.247\n","Accuracy 0.8910579085350037\n","Epoch: 12, Batch:   600, loss: 0.250\n","Accuracy 0.8911245465278625\n","Epoch: 12, Batch:   800, loss: 0.252\n","Accuracy 0.8912069201469421\n","Epoch: 12, Batch:  1000, loss: 0.235\n","Accuracy 0.8913195133209229\n","Epoch: 12, Batch:  1200, loss: 0.238\n","Accuracy 0.8914458155632019\n","Epoch: 12, Batch:  1400, loss: 0.244\n","Accuracy 0.8915621042251587\n","Epoch: 12, Batch:  1600, loss: 0.245\n","Accuracy 0.8916438221931458\n","Epoch: 12, Batch:  1800, loss: 0.251\n","Accuracy 0.8917131423950195\n","Epoch: 12, Batch:  2000, loss: 0.275\n","Accuracy 0.8917384147644043\n","Epoch: 12, Batch:  2200, loss: 0.242\n","Accuracy 0.8918195962905884\n","Epoch: 12, Batch:  2400, loss: 0.243\n","Accuracy 0.8918914794921875\n","Epoch: 12, Batch:  2600, loss: 0.256\n","Accuracy 0.8919840455055237\n","Epoch: 12, Batch:  2800, loss: 0.256\n","Accuracy 0.8920530676841736\n","Epoch: 12, Batch:  3000, loss: 0.256\n","Accuracy 0.8921158313751221\n","Epoch: 12, Batch:  3200, loss: 0.252\n","Accuracy 0.8921836614608765\n","Epoch: 12, Batch:  3400, loss: 0.263\n","Accuracy 0.8922396302223206\n","Epoch: 12, Batch:  3600, loss: 0.244\n","Accuracy 0.8923355340957642\n","Epoch: 13, Batch:   200, loss: 0.240\n","Accuracy 0.892476499080658\n","Epoch: 13, Batch:   400, loss: 0.254\n","Accuracy 0.8925482034683228\n","Epoch: 13, Batch:   600, loss: 0.229\n","Accuracy 0.892635703086853\n","Epoch: 13, Batch:   800, loss: 0.233\n","Accuracy 0.892714262008667\n","Epoch: 13, Batch:  1000, loss: 0.235\n","Accuracy 0.8928098082542419\n","Epoch: 13, Batch:  1200, loss: 0.253\n","Accuracy 0.8928828239440918\n","Epoch: 13, Batch:  1400, loss: 0.236\n","Accuracy 0.8929620385169983\n","Epoch: 13, Batch:  1600, loss: 0.261\n","Accuracy 0.893004298210144\n","Epoch: 13, Batch:  1800, loss: 0.247\n","Accuracy 0.8930542469024658\n","Epoch: 13, Batch:  2000, loss: 0.249\n","Accuracy 0.8931090235710144\n","Epoch: 13, Batch:  2200, loss: 0.259\n","Accuracy 0.8931673765182495\n","Epoch: 13, Batch:  2400, loss: 0.246\n","Accuracy 0.8932146430015564\n","Epoch: 13, Batch:  2600, loss: 0.236\n","Accuracy 0.8933114409446716\n","Epoch: 13, Batch:  2800, loss: 0.242\n","Accuracy 0.8933786749839783\n","Epoch: 13, Batch:  3000, loss: 0.250\n","Accuracy 0.8934466242790222\n","Epoch: 13, Batch:  3200, loss: 0.235\n","Accuracy 0.8935425281524658\n","Epoch: 13, Batch:  3400, loss: 0.253\n","Accuracy 0.8936079740524292\n","Epoch: 13, Batch:  3600, loss: 0.239\n","Accuracy 0.8937036991119385\n","Epoch: 14, Batch:   200, loss: 0.216\n","Accuracy 0.8938699960708618\n","Epoch: 14, Batch:   400, loss: 0.232\n","Accuracy 0.8939381837844849\n","Epoch: 14, Batch:   600, loss: 0.235\n","Accuracy 0.8940210342407227\n","Epoch: 14, Batch:   800, loss: 0.228\n","Accuracy 0.8941258788108826\n","Epoch: 14, Batch:  1000, loss: 0.241\n","Accuracy 0.8941922187805176\n","Epoch: 14, Batch:  1200, loss: 0.242\n","Accuracy 0.8942567706108093\n","Epoch: 14, Batch:  1400, loss: 0.244\n","Accuracy 0.8943133354187012\n","Epoch: 14, Batch:  1600, loss: 0.264\n","Accuracy 0.8943557739257812\n","Epoch: 14, Batch:  1800, loss: 0.242\n","Accuracy 0.8944448828697205\n","Epoch: 14, Batch:  2000, loss: 0.248\n","Accuracy 0.8945074081420898\n","Epoch: 14, Batch:  2200, loss: 0.235\n","Accuracy 0.894595205783844\n","Epoch: 14, Batch:  2400, loss: 0.249\n","Accuracy 0.8946871757507324\n","Epoch: 14, Batch:  2600, loss: 0.243\n","Accuracy 0.8947517275810242\n","Epoch: 14, Batch:  2800, loss: 0.244\n","Accuracy 0.8948266506195068\n","Epoch: 14, Batch:  3000, loss: 0.266\n","Accuracy 0.8948381543159485\n","Epoch: 14, Batch:  3200, loss: 0.240\n","Accuracy 0.8948929309844971\n","Epoch: 14, Batch:  3400, loss: 0.262\n","Accuracy 0.8949280977249146\n","Epoch: 14, Batch:  3600, loss: 0.261\n","Accuracy 0.894982099533081\n","Epoch: 15, Batch:   200, loss: 0.231\n","Accuracy 0.8950912952423096\n","Epoch: 15, Batch:   400, loss: 0.234\n","Accuracy 0.8951535820960999\n","Epoch: 15, Batch:   600, loss: 0.235\n","Accuracy 0.895236611366272\n","Epoch: 15, Batch:   800, loss: 0.222\n","Accuracy 0.8953130841255188\n","Epoch: 15, Batch:  1000, loss: 0.240\n","Accuracy 0.8953551650047302\n","Epoch: 15, Batch:  1200, loss: 0.246\n","Accuracy 0.8954108357429504\n","Epoch: 15, Batch:  1400, loss: 0.233\n","Accuracy 0.8954835534095764\n","Epoch: 15, Batch:  1600, loss: 0.231\n","Accuracy 0.8955475687980652\n","Epoch: 15, Batch:  1800, loss: 0.235\n","Accuracy 0.895615816116333\n","Epoch: 15, Batch:  2000, loss: 0.241\n","Accuracy 0.8956708908081055\n","Epoch: 15, Batch:  2200, loss: 0.263\n","Accuracy 0.8956981301307678\n","Epoch: 15, Batch:  2400, loss: 0.259\n","Accuracy 0.8957274556159973\n","Epoch: 15, Batch:  2600, loss: 0.242\n","Accuracy 0.8957792520523071\n","Epoch: 15, Batch:  2800, loss: 0.237\n","Accuracy 0.8958454132080078\n","Epoch: 15, Batch:  3000, loss: 0.245\n","Accuracy 0.8959110379219055\n","Epoch: 15, Batch:  3200, loss: 0.241\n","Accuracy 0.8959717154502869\n","Epoch: 15, Batch:  3400, loss: 0.246\n","Accuracy 0.8960162997245789\n","Epoch: 15, Batch:  3600, loss: 0.246\n","Accuracy 0.8960728645324707\n","Epoch: 16, Batch:   200, loss: 0.206\n","Accuracy 0.8961990475654602\n","Epoch: 16, Batch:   400, loss: 0.242\n","Accuracy 0.8962466716766357\n","Epoch: 16, Batch:   600, loss: 0.250\n","Accuracy 0.8962884545326233\n","Epoch: 16, Batch:   800, loss: 0.224\n","Accuracy 0.8963781595230103\n","Epoch: 16, Batch:  1000, loss: 0.233\n","Accuracy 0.8964465260505676\n","Epoch: 16, Batch:  1200, loss: 0.238\n","Accuracy 0.8965187072753906\n","Epoch: 16, Batch:  1400, loss: 0.230\n","Accuracy 0.8965860605239868\n","Epoch: 16, Batch:  1600, loss: 0.228\n","Accuracy 0.896648645401001\n","Epoch: 16, Batch:  1800, loss: 0.238\n","Accuracy 0.8967151045799255\n","Epoch: 16, Batch:  2000, loss: 0.254\n","Accuracy 0.8967499732971191\n","Epoch: 16, Batch:  2200, loss: 0.235\n","Accuracy 0.8968124389648438\n","Epoch: 16, Batch:  2400, loss: 0.248\n","Accuracy 0.8968520760536194\n","Epoch: 16, Batch:  2600, loss: 0.239\n","Accuracy 0.8969095349311829\n","Epoch: 16, Batch:  2800, loss: 0.234\n","Accuracy 0.8969517350196838\n","Epoch: 16, Batch:  3000, loss: 0.239\n","Accuracy 0.8969915509223938\n","Epoch: 16, Batch:  3200, loss: 0.240\n","Accuracy 0.8970300555229187\n","Epoch: 16, Batch:  3400, loss: 0.247\n","Accuracy 0.8970599174499512\n","Epoch: 16, Batch:  3600, loss: 0.237\n","Accuracy 0.8970927596092224\n","Epoch: 17, Batch:   200, loss: 0.218\n","Accuracy 0.8972030878067017\n","Epoch: 17, Batch:   400, loss: 0.216\n","Accuracy 0.8972764611244202\n","Epoch: 17, Batch:   600, loss: 0.239\n","Accuracy 0.8973267078399658\n","Epoch: 17, Batch:   800, loss: 0.238\n","Accuracy 0.8973735570907593\n","Epoch: 17, Batch:  1000, loss: 0.223\n","Accuracy 0.8974272608757019\n","Epoch: 17, Batch:  1200, loss: 0.215\n","Accuracy 0.8974938988685608\n","Epoch: 17, Batch:  1400, loss: 0.249\n","Accuracy 0.8975163102149963\n","Epoch: 17, Batch:  1600, loss: 0.230\n","Accuracy 0.8975912928581238\n","Epoch: 17, Batch:  1800, loss: 0.256\n","Accuracy 0.897597074508667\n","Epoch: 17, Batch:  2000, loss: 0.249\n","Accuracy 0.8976259827613831\n","Epoch: 17, Batch:  2200, loss: 0.218\n","Accuracy 0.897697925567627\n","Epoch: 17, Batch:  2400, loss: 0.251\n","Accuracy 0.8977263569831848\n","Epoch: 17, Batch:  2600, loss: 0.248\n","Accuracy 0.8977575898170471\n","Epoch: 17, Batch:  2800, loss: 0.254\n","Accuracy 0.8977925777435303\n","Epoch: 17, Batch:  3000, loss: 0.237\n","Accuracy 0.8978372812271118\n","Epoch: 17, Batch:  3200, loss: 0.240\n","Accuracy 0.8978807330131531\n","Epoch: 17, Batch:  3400, loss: 0.235\n","Accuracy 0.8979101181030273\n","Epoch: 17, Batch:  3600, loss: 0.245\n","Accuracy 0.8979520201683044\n","Epoch: 18, Batch:   200, loss: 0.220\n","Accuracy 0.8980443477630615\n","Epoch: 18, Batch:   400, loss: 0.231\n","Accuracy 0.8980913758277893\n","Epoch: 18, Batch:   600, loss: 0.222\n","Accuracy 0.8981361985206604\n","Epoch: 18, Batch:   800, loss: 0.235\n","Accuracy 0.898195207118988\n","Epoch: 18, Batch:  1000, loss: 0.207\n","Accuracy 0.8982808589935303\n","Epoch: 18, Batch:  1200, loss: 0.246\n","Accuracy 0.898318886756897\n","Epoch: 18, Batch:  1400, loss: 0.222\n","Accuracy 0.8983519077301025\n","Epoch: 18, Batch:  1600, loss: 0.253\n","Accuracy 0.898372232913971\n","Epoch: 18, Batch:  1800, loss: 0.222\n","Accuracy 0.8984420299530029\n","Epoch: 18, Batch:  2000, loss: 0.213\n","Accuracy 0.8985152244567871\n","Epoch: 18, Batch:  2200, loss: 0.233\n","Accuracy 0.8985595107078552\n","Epoch: 18, Batch:  2400, loss: 0.231\n","Accuracy 0.8986082673072815\n","Epoch: 18, Batch:  2600, loss: 0.248\n","Accuracy 0.8986510634422302\n","Epoch: 18, Batch:  2800, loss: 0.230\n","Accuracy 0.8987067937850952\n","Epoch: 18, Batch:  3000, loss: 0.244\n","Accuracy 0.8987424969673157\n","Epoch: 18, Batch:  3200, loss: 0.246\n","Accuracy 0.8987686634063721\n","Epoch: 18, Batch:  3400, loss: 0.268\n","Accuracy 0.8987900018692017\n","Epoch: 18, Batch:  3600, loss: 0.230\n","Accuracy 0.8988437056541443\n","Epoch: 19, Batch:   200, loss: 0.216\n","Accuracy 0.8989365100860596\n","Epoch: 19, Batch:   400, loss: 0.224\n","Accuracy 0.8989911675453186\n","Epoch: 19, Batch:   600, loss: 0.227\n","Accuracy 0.8990381956100464\n","Epoch: 19, Batch:   800, loss: 0.233\n","Accuracy 0.8990849256515503\n","Epoch: 19, Batch:  1000, loss: 0.240\n","Accuracy 0.8991231918334961\n","Epoch: 19, Batch:  1200, loss: 0.236\n","Accuracy 0.8991612195968628\n","Epoch: 19, Batch:  1400, loss: 0.215\n","Accuracy 0.8992189764976501\n","Epoch: 19, Batch:  1600, loss: 0.231\n","Accuracy 0.8992619514465332\n","Epoch: 19, Batch:  1800, loss: 0.220\n","Accuracy 0.899323582649231\n","Epoch: 19, Batch:  2000, loss: 0.226\n","Accuracy 0.8993704915046692\n","Epoch: 19, Batch:  2200, loss: 0.257\n","Accuracy 0.8993839621543884\n","Epoch: 19, Batch:  2400, loss: 0.222\n","Accuracy 0.8994411826133728\n","Epoch: 19, Batch:  2600, loss: 0.221\n","Accuracy 0.8994891047477722\n","Epoch: 19, Batch:  2800, loss: 0.235\n","Accuracy 0.8995145559310913\n","Epoch: 19, Batch:  3000, loss: 0.252\n","Accuracy 0.8995327949523926\n","Epoch: 19, Batch:  3200, loss: 0.220\n","Accuracy 0.899588942527771\n","Epoch: 19, Batch:  3400, loss: 0.231\n","Accuracy 0.8996191620826721\n","Epoch: 19, Batch:  3600, loss: 0.235\n","Accuracy 0.8996589183807373\n","Epoch: 20, Batch:   200, loss: 0.220\n","Accuracy 0.8997620940208435\n","Epoch: 20, Batch:   400, loss: 0.244\n","Accuracy 0.8997949957847595\n","Epoch: 20, Batch:   600, loss: 0.205\n","Accuracy 0.8998842835426331\n","Epoch: 20, Batch:   800, loss: 0.234\n","Accuracy 0.899922788143158\n","Epoch: 20, Batch:  1000, loss: 0.230\n","Accuracy 0.8999593257904053\n","Epoch: 20, Batch:  1200, loss: 0.227\n","Accuracy 0.8999913930892944\n","Epoch: 20, Batch:  1400, loss: 0.241\n","Accuracy 0.900024950504303\n","Epoch: 20, Batch:  1600, loss: 0.239\n","Accuracy 0.900054931640625\n","Epoch: 20, Batch:  1800, loss: 0.221\n","Accuracy 0.9001120924949646\n","Epoch: 20, Batch:  2000, loss: 0.225\n","Accuracy 0.900158703327179\n","Epoch: 20, Batch:  2200, loss: 0.219\n","Accuracy 0.9002135992050171\n","Epoch: 20, Batch:  2400, loss: 0.211\n","Accuracy 0.9002639055252075\n","Epoch: 20, Batch:  2600, loss: 0.232\n","Accuracy 0.9002978801727295\n","Epoch: 20, Batch:  2800, loss: 0.246\n","Accuracy 0.9003342390060425\n","Epoch: 20, Batch:  3000, loss: 0.234\n","Accuracy 0.900377094745636\n","Epoch: 20, Batch:  3200, loss: 0.214\n","Accuracy 0.9004214406013489\n","Epoch: 20, Batch:  3400, loss: 0.257\n","Accuracy 0.9004261493682861\n","Epoch: 20, Batch:  3600, loss: 0.226\n","Accuracy 0.9004575610160828\n","Epoch: 21, Batch:   200, loss: 0.220\n","Accuracy 0.900522768497467\n","Epoch: 21, Batch:   400, loss: 0.226\n","Accuracy 0.9005553722381592\n","Epoch: 21, Batch:   600, loss: 0.227\n","Accuracy 0.9005852937698364\n","Epoch: 21, Batch:   800, loss: 0.227\n","Accuracy 0.9006282687187195\n","Epoch: 21, Batch:  1000, loss: 0.215\n","Accuracy 0.9006932377815247\n","Epoch: 21, Batch:  1200, loss: 0.230\n","Accuracy 0.9007332921028137\n","Epoch: 21, Batch:  1400, loss: 0.212\n","Accuracy 0.9007844924926758\n","Epoch: 21, Batch:  1600, loss: 0.213\n","Accuracy 0.9008363485336304\n","Epoch: 21, Batch:  1800, loss: 0.232\n","Accuracy 0.9008731842041016\n","Epoch: 21, Batch:  2000, loss: 0.224\n","Accuracy 0.9009147882461548\n","Epoch: 21, Batch:  2200, loss: 0.238\n","Accuracy 0.9009334444999695\n","Epoch: 21, Batch:  2400, loss: 0.226\n","Accuracy 0.900978684425354\n","Epoch: 21, Batch:  2600, loss: 0.223\n","Accuracy 0.9010276794433594\n","Epoch: 21, Batch:  2800, loss: 0.256\n","Accuracy 0.9010258913040161\n","Epoch: 21, Batch:  3000, loss: 0.225\n","Accuracy 0.9010785222053528\n","Epoch: 21, Batch:  3200, loss: 0.199\n","Accuracy 0.9011445045471191\n","Epoch: 21, Batch:  3400, loss: 0.239\n","Accuracy 0.901171863079071\n","Epoch: 21, Batch:  3600, loss: 0.229\n","Accuracy 0.9011998772621155\n","Epoch: 22, Batch:   200, loss: 0.218\n","Accuracy 0.9012681841850281\n","Epoch: 22, Batch:   400, loss: 0.229\n","Accuracy 0.9013100266456604\n","Epoch: 22, Batch:   600, loss: 0.221\n","Accuracy 0.9013421535491943\n","Epoch: 22, Batch:   800, loss: 0.202\n","Accuracy 0.9013984799385071\n","Epoch: 22, Batch:  1000, loss: 0.242\n","Accuracy 0.9014255404472351\n","Epoch: 22, Batch:  1200, loss: 0.240\n","Accuracy 0.9014524817466736\n","Epoch: 22, Batch:  1400, loss: 0.235\n","Accuracy 0.9014917612075806\n","Epoch: 22, Batch:  1600, loss: 0.221\n","Accuracy 0.9015517830848694\n","Epoch: 22, Batch:  1800, loss: 0.205\n","Accuracy 0.9016092419624329\n","Epoch: 22, Batch:  2000, loss: 0.238\n","Accuracy 0.9016323685646057\n","Epoch: 22, Batch:  2200, loss: 0.239\n","Accuracy 0.9016560912132263\n","Epoch: 22, Batch:  2400, loss: 0.231\n","Accuracy 0.9016905426979065\n","Epoch: 22, Batch:  2600, loss: 0.222\n","Accuracy 0.9017232656478882\n","Epoch: 22, Batch:  2800, loss: 0.202\n","Accuracy 0.9017818570137024\n","Epoch: 22, Batch:  3000, loss: 0.246\n","Accuracy 0.9018080830574036\n","Epoch: 22, Batch:  3200, loss: 0.235\n","Accuracy 0.9018387794494629\n","Epoch: 22, Batch:  3400, loss: 0.248\n","Accuracy 0.9018639922142029\n","Epoch: 22, Batch:  3600, loss: 0.217\n","Accuracy 0.9019125699996948\n","Epoch: 23, Batch:   200, loss: 0.217\n","Accuracy 0.9019800424575806\n","Epoch: 23, Batch:   400, loss: 0.225\n","Accuracy 0.9020318388938904\n","Epoch: 23, Batch:   600, loss: 0.222\n","Accuracy 0.90206378698349\n","Epoch: 23, Batch:   800, loss: 0.239\n","Accuracy 0.9020835757255554\n","Epoch: 23, Batch:  1000, loss: 0.231\n","Accuracy 0.9021250009536743\n","Epoch: 23, Batch:  1200, loss: 0.230\n","Accuracy 0.902154266834259\n","Epoch: 23, Batch:  1400, loss: 0.223\n","Accuracy 0.902190089225769\n","Epoch: 23, Batch:  1600, loss: 0.220\n","Accuracy 0.9022272825241089\n","Epoch: 23, Batch:  1800, loss: 0.215\n","Accuracy 0.9022753834724426\n","Epoch: 23, Batch:  2000, loss: 0.250\n","Accuracy 0.9023002982139587\n","Epoch: 23, Batch:  2200, loss: 0.209\n","Accuracy 0.902344286441803\n","Epoch: 23, Batch:  2400, loss: 0.221\n","Accuracy 0.9023785591125488\n","Epoch: 23, Batch:  2600, loss: 0.214\n","Accuracy 0.9024221301078796\n","Epoch: 23, Batch:  2800, loss: 0.243\n","Accuracy 0.9024340510368347\n","Epoch: 23, Batch:  3000, loss: 0.237\n","Accuracy 0.9024627208709717\n","Epoch: 23, Batch:  3200, loss: 0.219\n","Accuracy 0.9024927020072937\n","Epoch: 23, Batch:  3400, loss: 0.230\n","Accuracy 0.9025196433067322\n","Epoch: 23, Batch:  3600, loss: 0.214\n","Accuracy 0.9025565981864929\n","Epoch: 24, Batch:   200, loss: 0.212\n","Accuracy 0.9026163816452026\n","Epoch: 24, Batch:   400, loss: 0.199\n","Accuracy 0.9026745557785034\n","Epoch: 24, Batch:   600, loss: 0.236\n","Accuracy 0.9027065634727478\n","Epoch: 24, Batch:   800, loss: 0.226\n","Accuracy 0.9027441143989563\n","Epoch: 24, Batch:  1000, loss: 0.218\n","Accuracy 0.9027844071388245\n","Epoch: 24, Batch:  1200, loss: 0.234\n","Accuracy 0.9028108716011047\n","Epoch: 24, Batch:  1400, loss: 0.225\n","Accuracy 0.902834415435791\n","Epoch: 24, Batch:  1600, loss: 0.235\n","Accuracy 0.9028514623641968\n","Epoch: 24, Batch:  1800, loss: 0.212\n","Accuracy 0.9028868675231934\n","Epoch: 24, Batch:  2000, loss: 0.232\n","Accuracy 0.9029228091239929\n","Epoch: 24, Batch:  2200, loss: 0.249\n","Accuracy 0.9029381275177002\n","Epoch: 24, Batch:  2400, loss: 0.235\n","Accuracy 0.902961790561676\n","Epoch: 24, Batch:  2600, loss: 0.234\n","Accuracy 0.9029818773269653\n","Epoch: 24, Batch:  2800, loss: 0.212\n","Accuracy 0.9030320048332214\n","Epoch: 24, Batch:  3000, loss: 0.213\n","Accuracy 0.9030707478523254\n","Epoch: 24, Batch:  3200, loss: 0.225\n","Accuracy 0.9031043648719788\n","Epoch: 24, Batch:  3400, loss: 0.256\n","Accuracy 0.9031155705451965\n","Epoch: 24, Batch:  3600, loss: 0.226\n","Accuracy 0.9031350612640381\n","Epoch: 25, Batch:   200, loss: 0.213\n","Accuracy 0.9031977653503418\n","Epoch: 25, Batch:   400, loss: 0.221\n","Accuracy 0.9032328724861145\n","Epoch: 25, Batch:   600, loss: 0.217\n","Accuracy 0.9032801985740662\n","Epoch: 25, Batch:   800, loss: 0.210\n","Accuracy 0.9033218622207642\n","Epoch: 25, Batch:  1000, loss: 0.221\n","Accuracy 0.9033557772636414\n","Epoch: 25, Batch:  1200, loss: 0.237\n","Accuracy 0.9033799171447754\n","Epoch: 25, Batch:  1400, loss: 0.209\n","Accuracy 0.9034245014190674\n","Epoch: 25, Batch:  1600, loss: 0.245\n","Accuracy 0.9034402370452881\n","Epoch: 25, Batch:  1800, loss: 0.228\n","Accuracy 0.9034613370895386\n","Epoch: 25, Batch:  2000, loss: 0.232\n","Accuracy 0.9034925103187561\n","Epoch: 25, Batch:  2200, loss: 0.222\n","Accuracy 0.903529703617096\n","Epoch: 25, Batch:  2400, loss: 0.242\n","Accuracy 0.9035423398017883\n","Epoch: 25, Batch:  2600, loss: 0.227\n","Accuracy 0.9035698175430298\n","Epoch: 25, Batch:  2800, loss: 0.218\n","Accuracy 0.9036099314689636\n","Epoch: 25, Batch:  3000, loss: 0.226\n","Accuracy 0.9036411046981812\n","Epoch: 25, Batch:  3200, loss: 0.222\n","Accuracy 0.9036762118339539\n","Epoch: 25, Batch:  3400, loss: 0.249\n","Accuracy 0.9036770462989807\n","Epoch: 25, Batch:  3600, loss: 0.225\n","Accuracy 0.9037119150161743\n","Epoch: 26, Batch:   200, loss: 0.223\n","Accuracy 0.9037653207778931\n","Epoch: 26, Batch:   400, loss: 0.201\n","Accuracy 0.9038124084472656\n","Epoch: 26, Batch:   600, loss: 0.220\n","Accuracy 0.9038467407226562\n","Epoch: 26, Batch:   800, loss: 0.232\n","Accuracy 0.903878927230835\n","Epoch: 26, Batch:  1000, loss: 0.205\n","Accuracy 0.903919517993927\n","Epoch: 26, Batch:  1200, loss: 0.231\n","Accuracy 0.9039415717124939\n","Epoch: 26, Batch:  1400, loss: 0.220\n","Accuracy 0.9039661288261414\n","Epoch: 26, Batch:  1600, loss: 0.223\n","Accuracy 0.9040010571479797\n","Epoch: 26, Batch:  1800, loss: 0.227\n","Accuracy 0.9040273427963257\n","Epoch: 26, Batch:  2000, loss: 0.226\n","Accuracy 0.9040535092353821\n","Epoch: 26, Batch:  2200, loss: 0.239\n","Accuracy 0.904076337814331\n","Epoch: 26, Batch:  2400, loss: 0.240\n","Accuracy 0.9040971398353577\n","Epoch: 26, Batch:  2600, loss: 0.224\n","Accuracy 0.9041197299957275\n","Epoch: 26, Batch:  2800, loss: 0.225\n","Accuracy 0.9041507244110107\n","Epoch: 26, Batch:  3000, loss: 0.232\n","Accuracy 0.9041627645492554\n","Epoch: 26, Batch:  3200, loss: 0.213\n","Accuracy 0.9041928648948669\n","Epoch: 26, Batch:  3400, loss: 0.258\n","Accuracy 0.9042016267776489\n","Epoch: 26, Batch:  3600, loss: 0.238\n","Accuracy 0.9042186737060547\n","Epoch: 27, Batch:   200, loss: 0.210\n","Accuracy 0.904260516166687\n","Epoch: 27, Batch:   400, loss: 0.195\n","Accuracy 0.9043105244636536\n","Epoch: 27, Batch:   600, loss: 0.205\n","Accuracy 0.9043508172035217\n","Epoch: 27, Batch:   800, loss: 0.239\n","Accuracy 0.9043775200843811\n","Epoch: 27, Batch:  1000, loss: 0.224\n","Accuracy 0.9044111967086792\n","Epoch: 27, Batch:  1200, loss: 0.225\n","Accuracy 0.9044332504272461\n","Epoch: 27, Batch:  1400, loss: 0.220\n","Accuracy 0.9044641256332397\n","Epoch: 27, Batch:  1600, loss: 0.206\n","Accuracy 0.9045023918151855\n","Epoch: 27, Batch:  1800, loss: 0.215\n","Accuracy 0.9045273065567017\n","Epoch: 27, Batch:  2000, loss: 0.232\n","Accuracy 0.9045571684837341\n","Epoch: 27, Batch:  2200, loss: 0.223\n","Accuracy 0.9045774936676025\n","Epoch: 27, Batch:  2400, loss: 0.233\n","Accuracy 0.9045996069908142\n","Epoch: 27, Batch:  2600, loss: 0.238\n","Accuracy 0.9046122431755066\n","Epoch: 27, Batch:  2800, loss: 0.227\n","Accuracy 0.9046472907066345\n","Epoch: 27, Batch:  3000, loss: 0.231\n","Accuracy 0.9046728610992432\n","Epoch: 27, Batch:  3200, loss: 0.249\n","Accuracy 0.9046934247016907\n","Epoch: 27, Batch:  3400, loss: 0.225\n","Accuracy 0.9047113656997681\n","Epoch: 27, Batch:  3600, loss: 0.217\n","Accuracy 0.9047372937202454\n","Epoch: 28, Batch:   200, loss: 0.225\n","Accuracy 0.9047812819480896\n","Epoch: 28, Batch:   400, loss: 0.181\n","Accuracy 0.904845654964447\n","Epoch: 28, Batch:   600, loss: 0.211\n","Accuracy 0.9048883318901062\n","Epoch: 28, Batch:   800, loss: 0.238\n","Accuracy 0.9049081206321716\n","Epoch: 28, Batch:  1000, loss: 0.237\n","Accuracy 0.9049254059791565\n","Epoch: 28, Batch:  1200, loss: 0.216\n","Accuracy 0.9049487709999084\n","Epoch: 28, Batch:  1400, loss: 0.205\n","Accuracy 0.9049884080886841\n","Epoch: 28, Batch:  1600, loss: 0.211\n","Accuracy 0.9050225019454956\n","Epoch: 28, Batch:  1800, loss: 0.216\n","Accuracy 0.9050442576408386\n","Epoch: 28, Batch:  2000, loss: 0.201\n","Accuracy 0.905091404914856\n","Epoch: 28, Batch:  2200, loss: 0.201\n","Accuracy 0.9051365256309509\n","Epoch: 28, Batch:  2400, loss: 0.243\n","Accuracy 0.9051362872123718\n","Epoch: 28, Batch:  2600, loss: 0.216\n","Accuracy 0.9051594734191895\n","Epoch: 28, Batch:  2800, loss: 0.230\n","Accuracy 0.9051837921142578\n","Epoch: 28, Batch:  3000, loss: 0.223\n","Accuracy 0.9052098393440247\n","Epoch: 28, Batch:  3200, loss: 0.231\n","Accuracy 0.9052339792251587\n","Epoch: 28, Batch:  3400, loss: 0.234\n","Accuracy 0.9052585959434509\n","Epoch: 28, Batch:  3600, loss: 0.242\n","Accuracy 0.9052658677101135\n","Epoch: 29, Batch:   200, loss: 0.216\n","Accuracy 0.9053166508674622\n","Epoch: 29, Batch:   400, loss: 0.207\n","Accuracy 0.9053575396537781\n","Epoch: 29, Batch:   600, loss: 0.199\n","Accuracy 0.9053971171379089\n","Epoch: 29, Batch:   800, loss: 0.229\n","Accuracy 0.9054253101348877\n","Epoch: 29, Batch:  1000, loss: 0.220\n","Accuracy 0.9054551720619202\n","Epoch: 29, Batch:  1200, loss: 0.221\n","Accuracy 0.9054672718048096\n","Epoch: 29, Batch:  1400, loss: 0.206\n","Accuracy 0.9054922461509705\n","Epoch: 29, Batch:  1600, loss: 0.224\n","Accuracy 0.9055089354515076\n","Epoch: 29, Batch:  1800, loss: 0.216\n","Accuracy 0.9055395722389221\n","Epoch: 29, Batch:  2000, loss: 0.233\n","Accuracy 0.9055590033531189\n","Epoch: 29, Batch:  2200, loss: 0.198\n","Accuracy 0.9055964350700378\n","Epoch: 29, Batch:  2400, loss: 0.234\n","Accuracy 0.9056104421615601\n","Epoch: 29, Batch:  2600, loss: 0.206\n","Accuracy 0.9056476354598999\n","Epoch: 29, Batch:  2800, loss: 0.234\n","Accuracy 0.9056638479232788\n","Epoch: 29, Batch:  3000, loss: 0.208\n","Accuracy 0.9056932926177979\n","Epoch: 29, Batch:  3200, loss: 0.212\n","Accuracy 0.9057202935218811\n","Epoch: 29, Batch:  3400, loss: 0.220\n","Accuracy 0.9057322144508362\n","Epoch: 29, Batch:  3600, loss: 0.241\n","Accuracy 0.9057521820068359\n","Epoch: 30, Batch:   200, loss: 0.194\n","Accuracy 0.9058163166046143\n","Epoch: 30, Batch:   400, loss: 0.208\n","Accuracy 0.9058462977409363\n","Epoch: 30, Batch:   600, loss: 0.239\n","Accuracy 0.9058567881584167\n","Epoch: 30, Batch:   800, loss: 0.229\n","Accuracy 0.9058825969696045\n","Epoch: 30, Batch:  1000, loss: 0.189\n","Accuracy 0.9059276580810547\n","Epoch: 30, Batch:  1200, loss: 0.219\n","Accuracy 0.9059646725654602\n","Epoch: 30, Batch:  1400, loss: 0.227\n","Accuracy 0.9059810638427734\n","Epoch: 30, Batch:  1600, loss: 0.222\n","Accuracy 0.9060109853744507\n","Epoch: 30, Batch:  1800, loss: 0.218\n","Accuracy 0.9060340523719788\n","Epoch: 30, Batch:  2000, loss: 0.226\n","Accuracy 0.9060603976249695\n","Epoch: 30, Batch:  2200, loss: 0.219\n","Accuracy 0.9060781598091125\n","Epoch: 30, Batch:  2400, loss: 0.218\n","Accuracy 0.9061015248298645\n","Epoch: 30, Batch:  2600, loss: 0.227\n","Accuracy 0.9061253666877747\n","Epoch: 30, Batch:  2800, loss: 0.217\n","Accuracy 0.9061514139175415\n","Epoch: 30, Batch:  3000, loss: 0.224\n","Accuracy 0.9061683416366577\n","Epoch: 30, Batch:  3200, loss: 0.205\n","Accuracy 0.9062108993530273\n","Epoch: 30, Batch:  3400, loss: 0.202\n","Accuracy 0.9062427282333374\n","Epoch: 30, Batch:  3600, loss: 0.220\n","Accuracy 0.9062683582305908\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RREjngOC_X2F","colab_type":"text"},"source":["##3.2.6 Test Accuracy\n","Let us look at how the network performs on the test dataset.  \n","Report your accuracy in your report.\n","\n"]},{"cell_type":"code","metadata":{"id":"QTyWZj2R_X2F","colab_type":"code","outputId":"4dc747e4-6ae5-4bac-cfc7-0d100398a527","executionInfo":{"status":"ok","timestamp":1570504420283,"user_tz":240,"elapsed":1411412,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","#TODO : Report this accuracy in your report.\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the 10000 test images: 88 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FCCVNEjV_X2G","colab_type":"text"},"source":["##3.2.7 Per Class accuracy\n","Now we see the test accuracy for each class in the test dataset.  \n","Report these accuracies in your report. Also identify the problematic classes.  \n","Can you explain why these classes have significantly lower accuracies compared to other classes? Record your responses in your LaTeX file.\n"]},{"cell_type":"code","metadata":{"id":"CO9h7Zbd_X2H","colab_type":"code","outputId":"a18dc853-e18c-4c82-da5a-d9ee6a71b88d","executionInfo":{"status":"ok","timestamp":1570504423915,"user_tz":240,"elapsed":1415036,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDBcZAN7Rdd_PLtFEJtpVRVM9VxYKPuZpSNDztW=s64","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["class_correct = list(0. for i in range(10))\n","class_total = list(0. for i in range(10))\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs, 1)\n","        c = (predicted == labels).squeeze()\n","        for i in range(4):\n","            label = labels[i]\n","            class_correct[label] += c[i].item()\n","            class_total[label] += 1\n","\n","\n","for i in range(10):\n","    print('Accuracy of %5s : %2d %%' % (\n","        classes[i], 100 * class_correct[i] / class_total[i]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of T-shirt/top : 81 %\n","Accuracy of Trouser : 99 %\n","Accuracy of Pullover : 86 %\n","Accuracy of Dress : 89 %\n","Accuracy of  Coat : 78 %\n","Accuracy of Sandal : 91 %\n","Accuracy of Shirt : 73 %\n","Accuracy of Sneaker : 94 %\n","Accuracy of   Bag : 98 %\n","Accuracy of Ankle boot : 95 %\n"],"name":"stdout"}]}]}